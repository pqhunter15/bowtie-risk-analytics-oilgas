# -*- coding: utf-8 -*-
"""Event_Barrier_normalization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HQETyfuvM8vCIbqjZL6q8vpgzFXS3XXo
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import os
print(os.listdir("/content/drive"))

import pandas as pd
import numpy as np

"""#Top event standardizing and filtering"""

df = pd.read_csv('/content/drive/MyDrive/Practicum/incident_outputs/flat_incidents_combinedV3.csv')

from google.colab import drive
drive.mount('/content/drive')

print(df.info())

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

model = SentenceTransformer("all-MiniLM-L6-v2")

texts = (
    df["incident__event__top_event"].fillna("") + " " +
    df["incident__event__incident_type"].fillna("")
).tolist()

embeddings = model.encode(texts, normalize_embeddings=True)

# Create a semantic anchor for "loss of containment"
anchor_text = (
    "loss of containment lopc loss of primary containment uncontrolled release unplanned release "
    "gas release gas leak hydrocarbon release vapor release toxic release chemical release "
    "oil discharge pollution unauthorized discharge drilling fluid discharge spill "
    "subsea leak pipeline leak pressure release well tree pressure release "
    "blowout loss of well control"
)

anchor_embedding = model.encode([anchor_text], normalize_embeddings=True)

similarities = cosine_similarity(embeddings, anchor_embedding).flatten()

df["loc_similarity"] = similarities

# Keep only high similarity rows
loc_df = df[df["loc_similarity"] > 0.45]

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2")

texts = (
    df["incident__event__top_event"].fillna("") + " " +
    df["incident__event__incident_type"].fillna("")
).tolist()

X = model.encode(texts, normalize_embeddings=True)

loc_anchors = [
    # Generic LOC framing
    "loss of containment",
    "loss of primary containment",
    "uncontrolled release",
    "unplanned release",
    "unauthorized discharge",
    "unplanned discharge",
    "pressure release",

    # Hydrocarbon / gas
    "gas release",
    "gas leak",
    "hydrocarbon release",
    "vapor release",
    "pipeline gas leak",
    "subsea gas leak",

    # Pollution / environmental discharge
    "oil discharge",
    "pollution unauthorized discharge",
    "discharge to sea",
    "drilling fluid discharge",
    "spill to environment",
    "subsea leak pollution",

    # Toxic / chemical / cryogenic releases
    "chemical release",
    "toxic release",
    "toxic gas release",
    "chlorine release",
    "hf release",
    "ammonia release",
    "cryogenic release",
    "asphyxiation from release",

    # Loss of well control (treated as LOC here)
    "loss of well control",
    "blowout",
    "well control incident uncontrolled release",

    # LOC + escalation outcomes (keep as positives because your labels include these)
    "vapor cloud explosion and fire",
    "explosion and fire from release",
    "tank explosion and chemical release",
    "fire and explosion from pipeline failure",
]

A = model.encode(loc_anchors, normalize_embeddings=True)

# similarity to each anchor (N x K)
S = cosine_similarity(X, A)

# Robust aggregates:
df["loc_sim_max"]  = S.max(axis=1)                 # best match to any anchor
df["loc_sim_mean"] = S.mean(axis=1)                # overall affinity
df["loc_sim_p90"]  = np.quantile(S, 0.90, axis=1)  # stable “high match” stat

# A simple first-pass filter:
loc_df = df[df["loc_sim_max"] > 0.35]

nonloc_anchors = [
    # Occupational safety / injuries
    "slip trip fall",
    "working at height fall",
    "struck by object",
    "caught in between pinch point",
    "hand injury laceration",
    "vehicle incident",
    "dropped object",
    "lifting operation crane incident",
    "line of fire",
    "personnel injury only",

    # Process upsets not necessarily releases
    "power failure blackout",
    "equipment malfunction no release",
    "mechanical failure without release",
    "alarm activation false alarm",

    # Structural / property damage without release
    "structural damage only",
    "equipment damage only",

    # Admin/compliance topics that can appear in text
    "systemic review recommendation",
    "audit inspection finding",
    "training procedure documentation",
]

A_pos = model.encode(loc_anchors, normalize_embeddings=True)
A_neg = model.encode(nonloc_anchors, normalize_embeddings=True)

S_pos = cosine_similarity(X, A_pos).max(axis=1)
S_neg = cosine_similarity(X, A_neg).max(axis=1)

df["loc_pos"] = S_pos
df["loc_neg"] = S_neg
df["loc_margin"] = df["loc_pos"] - df["loc_neg"]

# Example filter using both:
loc_df = df[(df["loc_pos"] > 0.45) & (df["loc_margin"] > 0.10)]

print(loc_df.head(20))

print(loc_df.shape)

print(texts)

texts_loc = (
    loc_df["incident__event__top_event"].fillna("") + " " +
    loc_df["incident__event__incident_type"].fillna("")
).tolist()

print(texts_loc)

print(loc_df['incident__event__top_event'].value_counts())

"""Creating summary DF for manual Review

#Joining Loc_df (incidents) with controls_df
"""

controls_df = pd.read_csv('/content/drive/MyDrive/Practicum/incident_outputs/controls_combinedV3.csv')

print(controls_df.info())

controls_loc_df = controls_df[
    controls_df["incident_id"].isin(loc_df["incident_id"])
].copy()

controls_loc_df.shape

print(controls_loc_df.info())

"""#Normailizng Barriers and creating barrier families:
After our full df is filtered for only loss of containment events, we need to normalize barrier names that are very similar to one another, as well as assign barrier families. Barrier names will likey not be passed to our model because of very high caridnality. Barrier familes however, may be predicitve and we can create a lower caridnality list. When doing this, we need to break our barriers into 4 groups, so that the families have meaningful groupings:
Prevention- admin
prevention- engineering
mitigative- admin
mitigative- engineering

Below is a pipeline which standardizes and groups the prevention-admin controls. This will need to be run on our full data set, and repeated (with different groundings) for the other 3 groups.

##Preventon Barriers

###Normalizing raw text
"""

import re

# -----------------------------
# 1) Filter to prevention controls
# -----------------------------
prev_df = (
    controls_loc_df.loc[controls_loc_df["side"].astype(str).str.strip().str.lower().eq("prevention")]
    .copy()
)

# Drop empty control names early (optional but recommended)
prev_df["control_name_raw"] = prev_df["name"].astype(str)
prev_df = prev_df.loc[prev_df["control_name_raw"].str.strip().ne("")].copy()

# Standardize barrier type string (so grouping is stable)
prev_df["control_barrier_type_norm"] = (
    prev_df["barrier_type"]
    .astype(str)
    .str.strip()
    .str.lower()
    .replace({"nan": "unknown", "none": "unknown", "": "unknown"})
)

# -----------------------------
# 2) Text normalization helpers
# -----------------------------
# Domain shorthand expansion (start small; you'll grow this dictionary as you see patterns)
ABBR_MAP = {
    "psv": "pressure safety valve",
    "prv": "pressure relief valve",
    "esd": "emergency shutdown",
    "bop": "blowout preventer",
    "scada": "supervisory control and data acquisition",
    "ppe": "personal protective equipment",
    "lopc": "loss of primary containment",
    "h2s": "hydrogen sulfide",
    "hse": "health safety environment",
    "sop": "standard operating procedure",
    "ptw": "permit to work",
}

ROLE_MAP = {
    "mitigation": "mitigate",
    "mitigating": "mitigate",
    "detection": "detect",
    "monitoring": "monitor",
    "isolation": "isolate",
    "shutdown": "shut down",
    "protection": "protect",
}

# Optional: de-noising tokens that often add little meaning
# (keep conservative at first — you can remove more later if needed)
STOP_PHRASES = [
    "program", "process", "system", "standard"
]

_punct_re = re.compile(r"[^\w\s]")
_ws_re = re.compile(r"\s+")

def _expand_abbreviations(text: str) -> str:
    for k, v in ABBR_MAP.items():
        text = re.sub(rf"\b{k}\b", v, text)
    for k, v in ROLE_MAP.items():
        text = re.sub(rf"\b{k}\b", v, text)
    return text

def normalize_control_name(s: str) -> str:
    s = str(s).strip().lower()
    if s in {"", "nan", "none"}:
        return ""

    # normalize common separators
    s = s.replace("&", " and ").replace("/", " ").replace("-", " ")

    # remove punctuation
    s = _punct_re.sub(" ", s)

    # expand abbreviations
    s = _expand_abbreviations(s)

    # collapse whitespace
    s = _ws_re.sub(" ", s).strip()

    # remove low-value phrases (whole-word)
    for p in STOP_PHRASES:
        s = re.sub(rf"\b{re.escape(p)}\b", "", s)

    # collapse whitespace again after removals
    s = _ws_re.sub(" ", s).strip()
    return s

def tokenize_norm(s: str) -> list[str]:
    # simple tokenization (we can upgrade later)
    return [t for t in s.split() if t]

# -----------------------------
# 3) Create normalized fields
# -----------------------------
prev_df["control_name_norm"] = prev_df["control_name_raw"].map(normalize_control_name)
prev_df["control_tokens"] = prev_df["control_name_norm"].map(tokenize_norm)
prev_df["control_len"] = prev_df["control_tokens"].map(len)

# Optionally drop rows where normalization results in empty string
prev_df = prev_df.loc[prev_df["control_name_norm"].ne("")].copy()

"""Normalizeing barrier_role"""

# -----------------------------
# 3b) Normalize barrier_role
# -----------------------------
prev_df["barrier_role_raw"] = prev_df["barrier_role"].astype(str)

prev_df["barrier_role_norm"] = (
    prev_df["barrier_role_raw"]
    .map(normalize_control_name)
)

prev_df["barrier_role_tokens"] = (
    prev_df["barrier_role_norm"]
    .map(tokenize_norm)
)

prev_df["barrier_role_len"] = (
    prev_df["barrier_role_tokens"]
    .map(len)
)

"""###Splitting out by barrier type"""

# -----------------------------
# 4) Break out groups by barrier type
# -----------------------------
# A dict of {barrier_type: dataframe_subset}
prev_groups = {
    barrier_type: g.copy()
    for barrier_type, g in prev_df.groupby("control_barrier_type_norm", dropna=False)
}

# Quick sanity prints (optional)
print("Prevention controls:", len(prev_df))
print("Barrier types:", len(prev_groups))
print("Top barrier types (by count):")
print(prev_df["control_barrier_type_norm"].value_counts().head(10))

"""###Admin Control Barriers"""

import pandas as pd

admin_dfs = []
for barrier_type in ["administrative", 'ppe', 'procedural']:
    df_subset = prev_groups.get(barrier_type)
    if df_subset is not None:
        admin_dfs.append(df_subset)

if not admin_dfs:
    raise ValueError(
        "No barrier type group found for 'administrative', 'ppe', or 'procedural'. "
        "Check prev_df['control_barrier_type_norm'].unique() for exact values."
    )

admin_df = pd.concat(admin_dfs, ignore_index=True)

print("Administrative prevention rows:", len(admin_df))
print("Unique raw control names:", admin_df["control_name_raw"].nunique())
print("\nTop 25 most frequent raw control names:")
print(admin_df["control_name_raw"].value_counts().head(25))

!pip -q install rapidfuzz

from rapidfuzz import fuzz
from collections import defaultdict

# -----------------------------
# 1) Build blocks so we only compare likely matches
# -----------------------------
# Blocking key: first 2 tokens + any "head noun" token if present
# (We'll refine as we see your data.)
HEAD_NOUNS = {
    "training", "procedure", "inspection", "audit", "permit", "meeting",
    "briefing", "policy", "sop", "checklist", "review", "drill",
    "risk", "assessment", "moc", "management", "change", "document",
    "record", "reporting", "supervision", "communication"
}

def block_keys(tokens: list[str]) -> set[str]:
    if not tokens:
        return set()
    keys = set()

    # first 1–2 tokens (helps with "hot work permit", "permit to work", etc.)
    keys.add(tokens[0])
    if len(tokens) >= 2:
        keys.add(tokens[0] + "_" + tokens[1])

    # head noun presence
    for t in tokens:
        if t in HEAD_NOUNS:
            keys.add("HN_" + t)

    return keys

# Create index -> keys
admin_df = admin_df.reset_index(drop=True).copy()
admin_df["block_keys"] = admin_df["control_tokens"].map(block_keys)

# Invert blocks: key -> list of row indices
block_index = defaultdict(list)
for i, keys in enumerate(admin_df["block_keys"]):
    for k in keys:
        block_index[k].append(i)

# -----------------------------
# 2) Compare within blocks using fuzzy metrics
# -----------------------------
# Thresholds (we’ll tune after looking at results):
LEX_THRESH = 90  # token_set_ratio is pretty strict at 90+
PARTIAL_THRESH = 92

pairs = []
seen = set()

names_norm = admin_df["control_name_norm"].tolist()
names_raw  = admin_df["control_name_raw"].tolist()

for k, idxs in block_index.items():
    if len(idxs) < 2:
        continue

    # compare each pair inside the block
    idxs_sorted = sorted(idxs)
    for a_pos in range(len(idxs_sorted)):
        i = idxs_sorted[a_pos]
        for b_pos in range(a_pos + 1, len(idxs_sorted)):
            j = idxs_sorted[b_pos]

            # de-dup across overlapping blocks
            key = (i, j)
            if key in seen:
                continue
            seen.add(key)

            s1 = names_norm[i]
            s2 = names_norm[j]

            # Skip if one is too short — tends to create noisy matches
            if len(s1) < 6 or len(s2) < 6:
                continue

            # Fuzzy scores
            tsr = fuzz.token_set_ratio(s1, s2)
            pr  = fuzz.partial_ratio(s1, s2)

            if tsr >= LEX_THRESH or pr >= PARTIAL_THRESH:
                pairs.append({
                    "i": i,
                    "j": j,
                    "block_key": k,
                    "raw_i": names_raw[i],
                    "raw_j": names_raw[j],
                    "norm_i": s1,
                    "norm_j": s2,
                    "token_set_ratio": tsr,
                    "partial_ratio": pr
                })

pairs_df = pd.DataFrame(pairs).sort_values(
    by=["token_set_ratio", "partial_ratio"],
    ascending=False
)

print("Candidate pairs found:", len(pairs_df))
pairs_df.head(30)

import networkx as nx

G = nx.Graph()
G.add_nodes_from(range(len(admin_df)))

for _, r in pairs_df.iterrows():
    # You can enforce stricter condition here if needed
    G.add_edge(int(r["i"]), int(r["j"]), weight=float(r["token_set_ratio"]))

components = list(nx.connected_components(G))

# Remove singletons
clusters = [c for c in components if len(c) >= 2]
clusters = sorted(clusters, key=len, reverse=True)

print("Non-singleton clusters:", len(clusters))
print("Top 10 cluster sizes:", [len(c) for c in clusters[:10]])

def show_cluster(cluster_idx: int, n=25):
    c = list(clusters[cluster_idx])
    sub = admin_df.loc[c, ["control_name_raw", "control_name_norm"]].copy()
    return sub.head(n)

# Example: inspect the biggest cluster
show_cluster(5, n=50)

"""####Semantic Embedding"""

!pip -q install sentence-transformers

from sentence_transformers import SentenceTransformer

# admin_df assumed from prior step:
# admin_df = prev_groups["administrative"].reset_index(drop=True).copy()

model = SentenceTransformer("all-MiniLM-L6-v2")

texts = admin_df["control_name_norm"].fillna("").astype(str).tolist()

# Normalize embeddings so cosine similarity = dot product
emb = model.encode(texts, normalize_embeddings=True, show_progress_bar=True)

# Store for later
admin_df["embed_idx"] = np.arange(len(admin_df))

from collections import defaultdict
from rapidfuzz import fuzz

SEM_THRESH = 0.84   # conservative; tune later if too few matches
LEX_BACKSTOP = 86   # if lexical is already strong, allow slightly lower semantic

ACTION_TOKENS = {
    "training","audit","inspection","permit","checklist","review","drill","briefing",
    "meeting","procedure","policy","sop","reporting","supervision","communication",
    "assessment","risk","moc","change","management"
}

def action_signature(tokens: list[str]) -> set[str]:
    return {t for t in tokens if t in ACTION_TOKENS}

# Build a block index again (reusing your block_keys function from prior step)
# admin_df["block_keys"] should already exist from the fuzzy step.
block_index = defaultdict(list)
for i, keys in enumerate(admin_df["block_keys"]):
    for k in keys:
        block_index[k].append(i)

# Precompute action signatures
admin_actions = admin_df["control_tokens"].map(action_signature).tolist()

semantic_pairs = []
seen = set()

for k, idxs in block_index.items():
    if len(idxs) < 2:
        continue

    idxs = sorted(set(idxs))
    # Compare within each block
    for a_pos in range(len(idxs)):
        i = idxs[a_pos]
        for b_pos in range(a_pos + 1, len(idxs)):
            j = idxs[b_pos]

            if (i, j) in seen:
                continue
            seen.add((i, j))

            # dot product = cosine similarity because embeddings normalized
            sim = float(np.dot(emb[i], emb[j]))

            if sim < (SEM_THRESH - 0.02):  # quick reject
                continue

            # Guardrail: require shared action concept OR strong lexical evidence
            a_sig = admin_actions[i]
            b_sig = admin_actions[j]
            shared_action = len(a_sig.intersection(b_sig)) > 0

            s1 = admin_df.loc[i, "control_name_norm"]
            s2 = admin_df.loc[j, "control_name_norm"]
            tsr = fuzz.token_set_ratio(s1, s2)

            # Conservative acceptance conditions
            accept = (sim >= SEM_THRESH and shared_action) or (sim >= SEM_THRESH and tsr >= LEX_BACKSTOP)

            if accept:
                semantic_pairs.append({
                    "i": i,
                    "j": j,
                    "block_key": k,
                    "cosine_sim": sim,
                    "token_set_ratio": tsr,
                    "raw_i": admin_df.loc[i, "control_name_raw"],
                    "raw_j": admin_df.loc[j, "control_name_raw"],
                    "norm_i": s1,
                    "norm_j": s2,
                    "shared_action": shared_action
                })

semantic_pairs_df = (
    pd.DataFrame(semantic_pairs)
    .sort_values(by=["cosine_sim", "token_set_ratio"], ascending=False)
)

print("Semantic candidate pairs found:", len(semantic_pairs_df))
semantic_pairs_df.head(30)

import networkx as nx

G = nx.Graph()
G.add_nodes_from(range(len(admin_df)))

# Add fuzzy edges (strict)
for _, r in pairs_df.iterrows():
    G.add_edge(int(r["i"]), int(r["j"]), edge_type="fuzzy",
               w=float(r["token_set_ratio"]))

# Add semantic edges (conservative)
for _, r in semantic_pairs_df.iterrows():
    G.add_edge(int(r["i"]), int(r["j"]), edge_type="semantic",
               w=float(r["cosine_sim"]))

components = list(nx.connected_components(G))
clusters = [c for c in components if len(c) >= 2]
singletons = [c for c in components if len(c) == 1]

clusters = sorted(clusters, key=len, reverse=True)

print("Clusters (size>=2):", len(clusters))
print("Singletons:", len(singletons))
print("Top 10 cluster sizes:", [len(c) for c in clusters[:10]])

def show_cluster(cluster_idx: int, n=50):
    c = list(clusters[cluster_idx])
    sub = admin_df.loc[c, ["control_name_raw", "control_name_norm"]].copy()
    return sub.sort_values("control_name_norm").head(n)

show_cluster(0, n=50)

show_cluster(1, n=50)

"""####Labeling Clusters"""

import hashlib

# components from the last step
# components = list(nx.connected_components(G))

# Build a cluster_id per row index
cluster_id_by_idx = {}

for comp in components:
    idxs = sorted(list(comp))
    # stable cluster id (hash of member indices)
    h = hashlib.md5(",".join(map(str, idxs)).encode("utf-8")).hexdigest()[:10]
    cid = f"admin_prev_{h}"
    for i in idxs:
        cluster_id_by_idx[i] = cid

admin_df["cluster_id"] = admin_df["embed_idx"].map(cluster_id_by_idx)
admin_df["cluster_size"] = admin_df.groupby("cluster_id")["cluster_id"].transform("size")

def choose_canonical(group: pd.DataFrame) -> str:
    # Most frequent raw name
    counts = group["control_name_raw"].value_counts()
    top_count = counts.iloc[0]
    top_names = counts[counts == top_count].index.tolist()

    if len(top_names) == 1:
        return top_names[0]

    # Tie-breaker: shortest normalized among tied raw names
    sub = group[group["control_name_raw"].isin(top_names)].copy()
    sub["norm_len"] = sub["control_name_norm"].astype(str).str.len()
    return sub.sort_values(["norm_len", "control_name_raw"]).iloc[0]["control_name_raw"]

# Compute canonical name per cluster
canon_map = (
    admin_df.groupby("cluster_id", as_index=False)
    .apply(lambda g: pd.Series({"standardized_control_name": choose_canonical(g)}))
    .reset_index(drop=True)
)

admin_df = admin_df.merge(canon_map, on="cluster_id", how="left")

admin_df.loc[admin_df["cluster_size"] == 1, "standardized_control_name"] = admin_df.loc[
    admin_df["cluster_size"] == 1, "control_name_raw"
]

admin_map = admin_df[[
    "side",
    "control_barrier_type_norm",
    "control_name_raw",
    "control_name_norm",
    "cluster_id",
    "cluster_size",
    "standardized_control_name"
]].copy()

# Optional: a simple confidence proxy (bigger cluster -> more confident)
admin_map["confidence"] = admin_map["cluster_size"].map(lambda n: "high" if n >= 5 else ("medium" if n >= 2 else "singleton"))

admin_map = admin_map.sort_values(
    ["cluster_size", "standardized_control_name", "control_name_norm"],
    ascending=[False, True, True]
)

admin_map.head(50)

print(admin_df['standardized_control_name'].value_counts())

"""While clustering revealed patterns in the barrier names, we will not use this to further transform barrier names. Instead, we will create a lower cardinaluty 'barrier_family' feature which groups barriers based on commonalities.

#### Creating a broader admin families feature to group singletons
"""

import re
import numpy as np
from sentence_transformers import SentenceTransformer

ABBR_MAP = {
    # common O&G / industrial safety abbreviations
    "psv": "pressure safety valve",
    "prv": "pressure relief valve",
    "esd": "emergency shutdown",
    "bop": "blowout preventer",
    "scada": "supervisory control and data acquisition",
    "ppe": "personal protective equipment",
    "lopc": "loss of primary containment",
    "h2s": "hydrogen sulfide",
    "hse": "health safety environment",
    "sop": "standard operating procedure",
    "ptw": "permit to work",
    "jsa": "job safety analysis",
    "jha": "job hazard analysis",
    "jsea": "job safety environmental analysis",
    "swa": "stop work authority",
    "loto": "lockout tagout",
    "moc": "management of change",
    "psm": "process safety management",
    "rmp": "risk management plan",
    "osha": "occupational safety and health administration",
    "sce": "safety critical element",
}

def expand_abbreviations(text: str) -> str:
    """Replace whole-word abbreviations only."""
    t = text
    for k, v in ABBR_MAP.items():
        t = re.sub(rf"\b{k}\b", v, t)
    return t


_punct_re = re.compile(r"[^\w\s]")
_ws_re = re.compile(r"\s+")

def normalize_for_family(s: str) -> str:
    """
    Minimal normalization:
    - lowercase
    - replace separators
    - remove punctuation
    - expand abbreviations
    - collapse whitespace
    Keep words like program/procedure/system/standard/policy/management etc.
    """
    s = str(s).strip().lower()
    if s in {"", "nan", "none"}:
        return ""

    s = s.replace("&", " and ").replace("/", " ").replace("-", " ")
    s = _punct_re.sub(" ", s)
    s = expand_abbreviations(s)
    s = _ws_re.sub(" ", s).strip()
    return s

def to_tokens(x: str) -> list[str]:
    return [t for t in str(x).split() if t]

# --- keep each field intact ---
admin_df["control_name_family_norm"] = admin_df["control_name_raw"].map(normalize_for_family)
admin_df["control_name_tokens"] = admin_df["control_name_family_norm"].map(to_tokens)

# NEW: normalize barrier_role as well (intact)
admin_df["barrier_role_raw"] = admin_df["barrier_role"].astype(str)
admin_df["barrier_role_family_norm"] = admin_df["barrier_role_raw"].map(normalize_for_family)
admin_df["barrier_role_tokens"] = admin_df["barrier_role_family_norm"].map(to_tokens)

# NEW: combined text used for matching/embedding (Option B)
admin_df["family_match_text_norm"] = (
    admin_df["control_name_family_norm"].fillna("").astype(str) + " " +
    admin_df["barrier_role_family_norm"].fillna("").astype(str)
).str.strip()

admin_df["family_match_tokens"] = admin_df["family_match_text_norm"].map(to_tokens)

"""Note: Barrier family taxonomies were created by passing the list of normalized barrier names to Chat GPT and asking it to create a broad group of families. Manual review was performed to ensure groundedness of results."""

#defining admin families
ADMIN_FAMILIES = {
    "hazard_analysis": [
        "process hazard analysis", "hazard analysis", "risk assessment",
        "job safety analysis", "job hazard analysis", "job safety environmental analysis",
        "hazard identification", "pre job risk assessment",
        # role-ish additions
        "identify hazards", "assess risk", "risk review", "hazard review"
    ],
    "permit_to_work": [
        "permit to work", "hot work permit", "confined space permit",
        "work authorization", "authorization to work",
        # role-ish additions
        "authorize work", "issue permit", "permit approval", "work permit approval"
    ],
    "procedures_work_instructions": [
        "standard operating procedure", "procedure", "work instruction",
        "installation procedure", "closure procedure", "torque specification",
        "line item checklist", "checklist",
        # role-ish additions
        "follow procedure", "work instruction compliance", "procedure compliance", "use checklist"
    ],
    "supervision_monitoring": [
        "supervision", "supervisor oversight", "operator supervision",
        "monitoring", "field oversight", "watchstander monitoring",
        # role-ish additions
        "monitor operations", "provide oversight", "supervise work", "continuous monitoring"
    ],
    "verification_independent_check": [
        "verification", "independent check", "two person check", "peer check",
        "double check", "double block and bleed verification", "alignment verification",
        # role-ish additions
        "verify alignment", "independent verification", "second check", "peer verification"
    ],
    "regulatory_psm_compliance": [
        "process safety management", "risk management plan",
        "regulatory compliance", "osha standard", "psm standard", "rmp coverage",
        "safety management system", "compliance requirement",
        # role-ish additions
        "meet regulatory requirements", "psm compliance", "compliance management", "regulatory requirement"
    ],
    "audit_assurance": [
        "audit", "inspection program", "assurance program",
        "management of audits", "inspection management",
        # role-ish additions
        "conduct audit", "perform inspection", "assurance review", "audit program"
    ],
    "incident_reporting_learning": [
        "incident reporting", "near miss reporting",
        "lessons learned", "learning system",
        # role-ish additions
        "report incident", "report near miss", "share lessons learned", "learning from incidents"
    ],
    "approval_change_control": [
        "approval process", "authorization", "management approval",
        "trial approval", "chemical trial approval", "change approval",
        "management of change",
        # role-ish additions
        "approve change", "change authorization", "moc approval", "management approval"
    ],
    "operating_limits_admin_controls": [
        "operating limit", "maximum safe fill height", "safe fill limit",
        "operating envelope", "safe venting", "administrative safeguard",
        # role-ish additions
        "maintain operating limits", "do not exceed limit", "operating envelope compliance", "restrict operations"
    ],
    "planning_coordination": [
        "pre job planning", "work planning", "job planning",
        "task coordination", "personnel pairing", "coordination meeting",
        # role-ish additions
        "plan the job", "coordinate work", "pre job plan", "coordinate personnel"
    ],
    "communication_handover": [
        "toolbox talk", "pre job briefing", "shift handover",
        "radio communication", "handover communication",
        # role-ish additions
        "conduct toolbox talk", "pre job brief", "shift handover briefing", "communicate status"
    ],
    "ppe_compliance": [
        "personal protective equipment", "ppe compliance", "ppe requirement",
        # role-ish additions
        "wear ppe", "ppe required", "ppe enforcement"
    ],
}

# STEP 4 — Multi-anchor embeddings for families (MAX similarity wins)
model = SentenceTransformer("all-MiniLM-L6-v2")

family_anchor_texts = []
family_lookup = []
for fam, phrases in ADMIN_FAMILIES.items():
    for p in phrases:
        family_anchor_texts.append(p)
        family_lookup.append(fam)

family_emb = model.encode(family_anchor_texts, normalize_embeddings=True, show_progress_bar=False)

# ----------------------------------------
# STEP 5 — Conservative keyword-first rules with "stem-ish" matching
# (Option B: expects you pass the COMBINED name+role text/tokens into assign_family)
# ----------------------------------------

import numpy as np  # ensure available

# Guardrail: keep taxonomy authoritative and prevent label drift
VALID_FAMILIES = set(ADMIN_FAMILIES.keys()) | {"other_admin", "unknown"}

def token_contains(tokens: list[str], root: str) -> bool:
    """True if any token contains the root substring (simple stem-like matching)."""
    return any(root in t for t in tokens)

def assign_family(text_norm: str, tokens: list[str], min_sim: float = 0.68) -> str:
    """
    Conservative family assignment:
    1) keyword/stem rules (high precision)
    2) embedding-to-family anchors (max similarity), thresholded
    3) fallback to other_admin
    """
    # Empty / missing text
    if not text_norm or str(text_norm).strip() == "":
        return "unknown"

    # ----- High-precision rule triggers -----
    # Hazard analysis / risk
    if (
        token_contains(tokens, "hazard")
        or token_contains(tokens, "pha")
        or token_contains(tokens, "risk")
        or token_contains(tokens, "jsa")
        or token_contains(tokens, "jha")
        or token_contains(tokens, "jsea")
    ):
        return "hazard_analysis"

    # Permits / authorization to work
    if token_contains(tokens, "permit") or token_contains(tokens, "ptw") or token_contains(tokens, "authoriz"):
        return "permit_to_work"

    # Procedures / instructions / checklists
    if (
        token_contains(tokens, "proced")
        or token_contains(tokens, "sop")
        or token_contains(tokens, "instruction")
        or token_contains(tokens, "checklist")
        or token_contains(tokens, "specif")
        or token_contains(tokens, "torque")
    ):
        return "procedures_work_instructions"

    # Supervision / monitoring
    if (
        token_contains(tokens, "supervis")
        or token_contains(tokens, "monitor")
        or token_contains(tokens, "oversight")
        or token_contains(tokens, "watch")
        or token_contains(tokens, "observe")
    ):
        return "supervision_monitoring"

    # Verification / independent check
    if (
        token_contains(tokens, "verif")
        or token_contains(tokens, "double")
        or token_contains(tokens, "two")
        or token_contains(tokens, "peer")
        or token_contains(tokens, "align")
        or token_contains(tokens, "independent")
        or token_contains(tokens, "witness")
        or token_contains(tokens, "validat")
    ):
        return "verification_independent_check"

    # Regulatory / PSM / compliance
    if (
        token_contains(tokens, "compli")
        or token_contains(tokens, "regulat")
        or token_contains(tokens, "osha")
        or token_contains(tokens, "psm")
        or token_contains(tokens, "rmp")
        or token_contains(tokens, "standard")
        or token_contains(tokens, "require")
    ):
        return "regulatory_psm_compliance"

    # Audit / inspection / assurance
    if (
        token_contains(tokens, "audit")
        or token_contains(tokens, "inspect")
        or token_contains(tokens, "assur")
        or token_contains(tokens, "assess")
        or token_contains(tokens, "review")
    ):
        return "audit_assurance"

    # Reporting / learning
    if (
        token_contains(tokens, "report")
        or token_contains(tokens, "incident")
        or token_contains(tokens, "learn")
        or token_contains(tokens, "near")
        or token_contains(tokens, "miss")
        or token_contains(tokens, "lesson")
    ):
        return "incident_reporting_learning"

    # Approval / change control / MOC
    if (
        token_contains(tokens, "approv")
        or token_contains(tokens, "authoriz")
        or token_contains(tokens, "trial")
        or token_contains(tokens, "moc")
        or (token_contains(tokens, "management") and token_contains(tokens, "change"))
    ):
        return "approval_change_control"

    # Operating limits / administrative safeguards
    if (
        token_contains(tokens, "limit")
        or token_contains(tokens, "maximum")
        or token_contains(tokens, "envelope")
        or token_contains(tokens, "vent")
        or token_contains(tokens, "restrict")
        or token_contains(tokens, "exceed")
    ):
        return "operating_limits_admin_controls"

    # Planning / coordination
    if (
        token_contains(tokens, "plan")
        or token_contains(tokens, "coordin")
        or token_contains(tokens, "pair")
        or token_contains(tokens, "schedule")
        or token_contains(tokens, "prep")
    ):
        return "planning_coordination"

    # Communications / handover
    if (
        token_contains(tokens, "toolbox")
        or token_contains(tokens, "brief")
        or token_contains(tokens, "handover")
        or token_contains(tokens, "radio")
        or token_contains(tokens, "commun")
        or token_contains(tokens, "shift")
        or token_contains(tokens, "logbook")
    ):
        return "communication_handover"

    # PPE compliance
    if token_contains(tokens, "ppe"):
        return "ppe_compliance"

    # ----- Embedding fallback -----
    # Uses family_emb and family_lookup created in Step 4 from ADMIN_FAMILIES
    v = model.encode([text_norm], normalize_embeddings=True, show_progress_bar=False)[0]
    sims = family_emb @ v

    best_idx = int(np.argmax(sims))
    best_sim = float(sims[best_idx])

    if best_sim >= min_sim:
        fam = family_lookup[best_idx]
        return fam if fam in VALID_FAMILIES else "other_admin"

    return "other_admin"

# ----------------------------------------
# APPLY — make sure you use the COMBINED name+role fields (Option B)
# Requires:
#   admin_df["family_match_text_norm"]   = normalized(name + " " + barrier_role)
#   admin_df["family_match_tokens"]      = tokens(name + barrier_role)
# ----------------------------------------

admin_df["admin_family"] = admin_df.apply(
    lambda r: assign_family(
        text_norm=r.get("family_match_text_norm", ""),
        tokens=r.get("family_match_tokens", []),
        min_sim=0.68,
    ),
    axis=1
)

# ----------------------------------------
# STEP 7 — diagnostics for threshold tuning
# ----------------------------------------
# See how close "other_admin" items are to *any* family anchor
def best_family_sim(text_norm: str) -> float:
    if not text_norm:
        return 0.0
    v = model.encode([text_norm], normalize_embeddings=True, show_progress_bar=False)[0]
    return float(np.max(family_emb @ v))

admin_df["best_family_sim"] = admin_df["control_name_family_norm"].map(best_family_sim)

print("Family distribution (proportions):")
print(admin_df["admin_family"].value_counts(normalize=True))

print("\nSimilarity stats for items still labeled other_admin:")
print(admin_df.loc[admin_df["admin_family"] == "other_admin", "best_family_sim"].describe())

# STEP 8 — (Optional) Create a reusable mapping table
# ----------------------------------------
admin_family_map = admin_df[[
    "side",
    "control_barrier_type_norm",
    "control_name_raw",
    "control_name_family_norm",
    "admin_family",
    "best_family_sim"
]].copy()

# Sort for review
admin_family_map = admin_family_map.sort_values(
    ["admin_family", "best_family_sim"],
    ascending=True
)

# Preview top rows
admin_family_map.head(50)

# Top families by count
admin_df["admin_family"].value_counts().head(12)

# Sample 20 examples from each of the top 6 families
top_fams = admin_df["admin_family"].value_counts().head(6).index.tolist()

for fam in top_fams:
    print("\n=== ", fam, "===")
    display(
        admin_df.loc[admin_df["admin_family"] == fam, ["control_name_raw"]]
        .sample(min(20, (admin_df["admin_family"] == fam).sum()), random_state=42)
    )

# Anything assigned by embedding fallback with low similarity should be reviewed
review_df = admin_df.loc[
    (admin_df["admin_family"] != "unknown") &
    (admin_df["best_family_sim"] < 0.66),
    ["control_name_raw", "admin_family", "best_family_sim"]
].sort_values("best_family_sim")

review_df.head(50)

def family_confidence(sim: float) -> str:
    if sim >= 0.78:
        return "high"
    if sim >= 0.70:
        return "medium"
    if sim >= 0.64:
        return "low"
    return "very_low"

admin_df["family_confidence"] = admin_df["best_family_sim"].map(family_confidence)

admin_df[["admin_family", "family_confidence"]].value_counts(normalize=True).head(20)

"""##engineering- prevention controls - Barrier Family pipeline"""

engineering_dfs = []
for barrier_type in ["engineering"]:
    df_subset = prev_groups.get(barrier_type)
    if df_subset is not None:
        engineering_dfs.append(df_subset)

engineering_df = pd.concat(engineering_dfs, ignore_index=True)

# Apply family-specific normalization to engineering_df
# Explicitly ensure raw name and role columns exist for this specific dataframe
engineering_df["control_name_raw"] = engineering_df["name"].astype(str)
engineering_df["barrier_role_raw"] = engineering_df["barrier_role"].astype(str)

engineering_df["control_name_family_norm"] = engineering_df["control_name_raw"].map(normalize_for_family)
engineering_df["barrier_role_family_norm"] = engineering_df["barrier_role_raw"].map(normalize_for_family)

# Combine normalized name and role for family matching
engineering_df["family_match_text_norm"] = (
    engineering_df["control_name_family_norm"].fillna("").astype(str) + " " +
    engineering_df["barrier_role_family_norm"].fillna("").astype(str)
).str.strip()
engineering_df["family_match_tokens"] = engineering_df["family_match_text_norm"].map(to_tokens)

# Now assign match_tokens
engineering_df["match_tokens"] = engineering_df["family_match_tokens"]

"""Head noun candidate search"""

from collections import Counter

# Engineering-specific stop tokens (start conservative; expand after inspection)
STOP_TOKENS_ENG = {
    "and","or","the","a","an","to","of","for","in","on","with","by","from",
    "nan","none","unknown",
    # super-generic terms (often not helpful for blocking)
    "system","process","program","standard","procedure",
    "device","equipment","component","unit",
    "control","controls","barrier","safeguard"
}

def rank_head_noun_candidates(tokens_series, top_n=250):
    """
    Heuristic: tokens that appear frequently, and especially as the LAST token,
    are good candidates for 'head nouns' for blocking.
    """
    all_tokens = []
    last_tokens = []

    for toks in tokens_series.dropna():
        toks = [t for t in toks if t and t not in STOP_TOKENS_ENG and len(t) > 2]
        if not toks:
            continue
        all_tokens.extend(toks)
        last_tokens.append(toks[-1])

    all_counts = Counter(all_tokens)
    last_counts = Counter(last_tokens)

    # Weighted score: last-token occurrences get extra weight
    score = Counter(all_counts)
    for t, c in last_counts.items():
        score[t] += 2 * c

    return score.most_common(top_n)

candidates = rank_head_noun_candidates(engineering_df["match_tokens"], top_n=250)

# Inspect top candidates (manually copy the good ones into HEAD_NOUNS_ENG)
for t, s in candidates[:80]:
    print(f"{t:20s} {s}")

from collections import defaultdict
from rapidfuzz import fuzz

def block_keys(tokens: list[str], head_nouns: set[str]) -> set[str]:
    if not tokens:
        return set()

    keys = set()
    keys.add(tokens[0])
    if len(tokens) >= 2:
        keys.add(tokens[0] + "_" + tokens[1])

    for t in tokens:
        if t in head_nouns:
            keys.add("HN_" + t)

    return keys

HEAD_NOUNS_ENG = {
    # valves / isolation / relief
    "valve", "psv", "prv", "relief", "isolation", "isolate", "block", "bleed",
    "esd", "shutdown", "sdv", "bop",

    # containment / barriers
    "containment", "secondary", "bund", "berm", "dike", "liner", "drain", "sump",
    "seal", "gasket", "packing", "flange", "coupling", "hose", "fitting",

    # instrumentation / sensing / alarms
    "instrument", "instrumentation", "sensor", "transmitter", "switch", "gauge", "meter",
    "alarm", "annunciator", "trip", "interlock", "logic", "sis", "scada", "plc",

    # detection (gas/fire/etc.)
    "detector", "detection", "gas", "fire", "smoke", "flame", "h2s", "leak",

    # pressure/flow control
    "pressure", "flow", "level", "temperature", "control", "controller", "regulator",

    # mechanical integrity / hardware
    "guard", "shield", "barrier", "enclosure", "housing", "cover", "grating",
    "handrail", "ladder", "platform", "walkway",
    "vent", "venting", "flare", "stack", "rupture", "disk",

    # pumps/compressors/rotating
    "pump", "compressor", "motor", "engine", "turbine", "fan",

    # electrical / grounding
    "grounding", "bonding", "earthing", "breaker", "fuse", "interrupter", "gfcI",
    "cable", "wiring", "junction", "panel",

    # structural / piping
    "piping", "pipe", "vessel", "tank", "separator", "header", "manifold",
    "support", "bracket", "clamp",

    # ventilation / HVAC
    "ventilation", "hvac", "exhaust", "intake", "damper",

    # emergency response engineered systems
    "sprinkler", "deluge", "foam", "extinguisher", "suppression", "hydrant",

    # access control / physical
    "fence", "gate", "lock", "interlock", "signage"  # signage sometimes admin-ish but appears in eng sets
}

engineering_df = engineering_df.reset_index(drop=True).copy()
engineering_df["block_keys"] = engineering_df["match_tokens"].map(lambda toks: block_keys(toks, HEAD_NOUNS_ENG))

block_index = defaultdict(list)
for i, keys in enumerate(engineering_df["block_keys"]):
    for k in keys:
        block_index[k].append(i)

LEX_THRESH = 90
PARTIAL_THRESH = 92

pairs = []
seen = set()

names_norm = engineering_df["family_match_text_norm"].tolist()
names_raw  = engineering_df["control_name_raw"].tolist()

for k, idxs in block_index.items():
    if len(idxs) < 2:
        continue

    idxs_sorted = sorted(idxs)
    for a_pos in range(len(idxs_sorted)):
        i = idxs_sorted[a_pos]
        for b_pos in range(a_pos + 1, len(idxs_sorted)):
            j = idxs_sorted[b_pos]

            key = (i, j)
            if key in seen:
                continue
            seen.add(key)

            s1 = names_norm[i]
            s2 = names_norm[j]

            if len(s1) < 6 or len(s2) < 6:
                continue

            tsr = fuzz.token_set_ratio(s1, s2)
            pr  = fuzz.partial_ratio(s1, s2)

            if tsr >= LEX_THRESH or pr >= PARTIAL_THRESH:
                pairs.append({
                    "i": i,
                    "j": j,
                    "block_key": k,
                    "raw_i": names_raw[i],
                    "raw_j": names_raw[j],
                    "norm_i": s1,
                    "norm_j": s2,
                    "token_set_ratio": tsr,
                    "partial_ratio": pr
                })

pairs_df_eng = pd.DataFrame(pairs).sort_values(
    by=["token_set_ratio", "partial_ratio"],
    ascending=False
)

print("Engineering candidate pairs found:", len(pairs_df_eng))
pairs_df_eng.head(30)

# ============================================================
# ENGINEERING FAMILY ASSIGNMENT PIPELINE (Option B: name + role)
# Drop-in replacement for your ADMIN family pipeline
# ============================================================

import re
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer

# -----------------------------
# 0) Abbreviation map (reuse + add common engineering/instrument abbreviations)
# -----------------------------
ABBR_MAP = {
    # common O&G / industrial safety abbreviations
    "psv": "pressure safety valve",
    "prv": "pressure relief valve",
    "esd": "emergency shutdown",
    "sdv": "shutdown valve",
    "bop": "blowout preventer",
    "scada": "supervisory control and data acquisition",
    "plc": "programmable logic controller",
    "sis": "safety instrumented system",
    "psd": "process shutdown",
    "efd": "emergency flowline disconnect",  # keep/remove if irrelevant to your corpus
    "cctv": "closed circuit television",
    "ppe": "personal protective equipment",
    "lopc": "loss of primary containment",
    "h2s": "hydrogen sulfide",
    "hse": "health safety environment",
    "pfd": "process flow diagram",
    "pid": "piping and instrumentation diagram",
    "hvac": "heating ventilation and air conditioning",
    "f&amp;g": "fire and gas",  # handles HTML-escaped ampersand
    "fg": "fire gas",
}

def expand_abbreviations(text: str) -> str:
    """Replace whole-word abbreviations only."""
    t = text
    for k, v in ABBR_MAP.items():
        t = re.sub(rf"\b{k}\b", v, t)
    return t

_punct_re = re.compile(r"[^\w\s]")
_ws_re = re.compile(r"\s+")

def normalize_for_family(s: str) -> str:
    """
    Minimal normalization:
    - lowercase
    - replace separators
    - remove punctuation
    - expand abbreviations
    - collapse whitespace
    Keep engineering-relevant nouns/verbs (don't remove 'system', etc.)
    """
    s = str(s).strip().lower()
    if s in {"", "nan", "none"}:
        return ""

    s = s.replace("&", " and ").replace("/", " ").replace("-", " ")
    s = _punct_re.sub(" ", s)
    s = expand_abbreviations(s)
    s = _ws_re.sub(" ", s).strip()
    return s

def to_tokens(x: str) -> list[str]:
    return [t for t in str(x).split() if t]


# -----------------------------
# 1) Create normalized engineering fields (keep name + role intact)
# -----------------------------
# Requires engineering_df exists and has columns: name, barrier_role
engineering_df = engineering_df.copy()

engineering_df["control_name_raw"] = engineering_df["name"].astype(str)
engineering_df["barrier_role_raw"] = engineering_df["barrier_role"].astype(str)

engineering_df["control_name_family_norm"] = engineering_df["control_name_raw"].map(normalize_for_family)
engineering_df["control_name_tokens"] = engineering_df["control_name_family_norm"].map(to_tokens)

engineering_df["barrier_role_family_norm"] = engineering_df["barrier_role_raw"].map(normalize_for_family)
engineering_df["barrier_role_tokens"] = engineering_df["barrier_role_family_norm"].map(to_tokens)

# Option B combined text for matching + embeddings
engineering_df["family_match_text_norm"] = (
    engineering_df["control_name_family_norm"].fillna("").astype(str) + " " +
    engineering_df["barrier_role_family_norm"].fillna("").astype(str)
).str.strip()

engineering_df["family_match_tokens"] = engineering_df["family_match_text_norm"].map(to_tokens)


# -----------------------------
# 2) Define ENGINEERING family taxonomy (starter set; expand with your clustered results)
# -----------------------------
ENG_FAMILIES = {
    "isolation_shut_in": [
        "isolation valve", "shutdown valve", "emergency shutdown", "esd",
        "close valve", "shut in", "isolate flow", "block valve", "sdv", "bop",
        "actuated valve", "fail closed valve"
    ],
    "pressure_relief_overpressure_protection": [
        "pressure safety valve", "pressure relief valve", "relief valve", "psv", "prv",
        "rupture disk", "burst disk", "overpressure protection", "pressure relief"
    ],
    "safety_instrumented_systems_interlocks": [
        "safety instrumented system", "sis", "instrumented protection", "trip",
        "interlock", "logic solver", "plc", "shutdown logic", "process shutdown"
    ],
    "alarms_indication_warning": [
        "alarm", "high high alarm", "annunciator", "warning alarm",
        "indicator", "indication", "control room alarm", "audible alarm", "visual alarm"
    ],
    "gas_fire_leak_detection": [
        "fire and gas detection", "gas detector", "gas detection", "fire detection",
        "h2s detector", "smoke detector", "flame detector", "leak detection", "detector"
    ],
    "containment_secondary_containment": [
        "secondary containment", "bund", "berm", "dike", "drip pan",
        "spill containment", "curbing", "sump", "drain system", "containment"
    ],
    "mechanical_integrity_passive_barriers": [
        "flange", "gasket", "seal", "packing", "hose", "fitting",
        "pressure rated piping", "piping integrity", "corrosion resistant", "barrier"
    ],
    "venting_flaring_exhaust": [
        "vent", "venting", "flare", "flare system", "relief to flare",
        "exhaust ventilation", "vent stack"
    ],
    "guards_shields_physical_protection": [
        "machine guard", "guarding", "shield", "enclosure", "blast wall",
        "barrier guard", "protective cover", "physical barrier"
    ],
    "electrical_grounding_isolation": [
        "grounding", "bonding", "earthing", "circuit breaker", "fuse",
        "electrical isolation", "disconnect switch", "gfi", "gfci"
    ],
    "emergency_response_engineered_systems": [
        "deluge system", "sprinkler", "foam system", "fire suppression",
        "extinguisher system", "hydrant", "firewater"
    ],
    "process_control_instrumentation": [
        "pressure transmitter", "flow transmitter", "level transmitter", "temperature transmitter",
        "controller", "regulator", "control valve", "instrument", "sensor", "switch", "gauge", "meter"
    ],
}


# -----------------------------
# 3) Create multi-anchor embeddings for engineering families
# -----------------------------
model = SentenceTransformer("all-MiniLM-L6-v2")

eng_anchor_texts = []
eng_lookup = []
for fam, phrases in ENG_FAMILIES.items():
    for p in phrases:
        eng_anchor_texts.append(p)
        eng_lookup.append(fam)

eng_family_emb = model.encode(eng_anchor_texts, normalize_embeddings=True, show_progress_bar=False)


# -----------------------------
# 4) Conservative rule-first assignment + embedding fallback
# -----------------------------
VALID_ENG_FAMILIES = set(ENG_FAMILIES.keys()) | {"other_engineering", "unknown"}

def token_contains(tokens: list[str], root: str) -> bool:
    return any(root in t for t in tokens)

def assign_eng_family(text_norm: str, tokens: list[str], min_sim: float = 0.68) -> str:
    if not text_norm or str(text_norm).strip() == "":
        return "unknown"

    # ---- High-precision rules (engineering) ----

    # Isolation / shutdown / BOP / SDV
    if (
        token_contains(tokens, "isolat")
        or token_contains(tokens, "shut")
        or token_contains(tokens, "sdv")
        or token_contains(tokens, "esd")
        or token_contains(tokens, "bop")
        or (token_contains(tokens, "close") and token_contains(tokens, "valv"))
    ):
        return "isolation_shut_in"

    # Pressure relief / overpressure
    if (
        token_contains(tokens, "relief")
        or token_contains(tokens, "psv")
        or token_contains(tokens, "prv")
        or (token_contains(tokens, "rupture") and token_contains(tokens, "disk"))
        or token_contains(tokens, "overpress")
    ):
        return "pressure_relief_overpressure_protection"

    # SIS / interlocks / trips
    if (
        token_contains(tokens, "interlock")
        or token_contains(tokens, "sis")
        or token_contains(tokens, "trip")
        or token_contains(tokens, "logic")
        or token_contains(tokens, "plc")
        or token_contains(tokens, "shutdown")
    ):
        return "safety_instrumented_systems_interlocks"

    # Alarms / annunciation
    if (
        token_contains(tokens, "alarm")
        or token_contains(tokens, "annunci")
        or token_contains(tokens, "warning")
        or token_contains(tokens, "indicator")
        or token_contains(tokens, "indicat")
    ):
        return "alarms_indication_warning"

    # Fire / gas / leak detection
    if (
        token_contains(tokens, "detect")
        or token_contains(tokens, "detector")
        or token_contains(tokens, "leak")
        or token_contains(tokens, "h2s")
        or token_contains(tokens, "gas")
        or token_contains(tokens, "fire")
        or token_contains(tokens, "flame")
        or token_contains(tokens, "smoke")
    ):
        return "gas_fire_leak_detection"

    # Secondary containment / spill control
    if (
        token_contains(tokens, "contain")
        or token_contains(tokens, "bund")
        or token_contains(tokens, "berm")
        or token_contains(tokens, "dike")
        or token_contains(tokens, "sump")
        or token_contains(tokens, "drain")
        or token_contains(tokens, "spill")
    ):
        return "containment_secondary_containment"

    # Passive/mechanical integrity components
    if (
        token_contains(tokens, "gasket")
        or token_contains(tokens, "seal")
        or token_contains(tokens, "packing")
        or token_contains(tokens, "flange")
        or token_contains(tokens, "hose")
        or token_contains(tokens, "fitting")
        or token_contains(tokens, "piping")
        or token_contains(tokens, "corros")
    ):
        return "mechanical_integrity_passive_barriers"

    # Venting / flare
    if (
        token_contains(tokens, "vent")
        or token_contains(tokens, "flare")
        or token_contains(tokens, "stack")
        or token_contains(tokens, "exhaust")
    ):
        return "venting_flaring_exhaust"

    # Guards / shields / enclosures
    if (
        token_contains(tokens, "guard")
        or token_contains(tokens, "shield")
        or token_contains(tokens, "enclos")
        or token_contains(tokens, "cover")
        or token_contains(tokens, "blast")
    ):
        return "guards_shields_physical_protection"

    # Electrical grounding / isolation
    if (
        token_contains(tokens, "ground")
        or token_contains(tokens, "bond")
        or token_contains(tokens, "earth")
        or token_contains(tokens, "breaker")
        or token_contains(tokens, "fuse")
        or token_contains(tokens, "disconnect")
        or token_contains(tokens, "isolat") and token_contains(tokens, "electr")
    ):
        return "electrical_grounding_isolation"

    # Fire suppression engineered systems
    if (
        token_contains(tokens, "deluge")
        or token_contains(tokens, "sprink")
        or token_contains(tokens, "foam")
        or token_contains(tokens, "suppress")
        or token_contains(tokens, "hydrant")
        or token_contains(tokens, "firewater")
    ):
        return "emergency_response_engineered_systems"

    # Process control / instrumentation (transmitters, sensors, control valves, etc.)
    if (
        token_contains(tokens, "transmitt")
        or token_contains(tokens, "sensor")
        or token_contains(tokens, "switch")
        or token_contains(tokens, "gauge")
        or token_contains(tokens, "meter")
        or token_contains(tokens, "controller")
        or token_contains(tokens, "regulat")
        or (token_contains(tokens, "control") and token_contains(tokens, "valv"))
        or token_contains(tokens, "instrument")
    ):
        return "process_control_instrumentation"

    # ---- Embedding fallback ----
    v = model.encode([text_norm], normalize_embeddings=True, show_progress_bar=False)[0]
    sims = eng_family_emb @ v
    best_idx = int(np.argmax(sims))
    best_sim = float(sims[best_idx])

    if best_sim >= min_sim:
        fam = eng_lookup[best_idx]
        return fam if fam in VALID_ENG_FAMILIES else "other_engineering"

    return "other_engineering"


# -----------------------------
# 5) Apply to engineering_df (uses combined name+role fields)
# -----------------------------
engineering_df["eng_family"] = engineering_df.apply(
    lambda r: assign_eng_family(
        text_norm=r.get("family_match_text_norm", ""),
        tokens=r.get("family_match_tokens", []),
        min_sim=0.68
    ),
    axis=1
)

# Optional quick QA
print(engineering_df["eng_family"].value_counts(dropna=False).head(20))

print(engineering_df.head())

"""##Mitigations Transformation"""

# -----------------------------
# 1) Filter to mitigation controls
# -----------------------------
mit_df = (
    controls_loc_df.loc[
        controls_loc_df["side"].astype(str).str.strip().str.lower().eq("mitigation")
    ].copy()
)

# Drop empty control names
mit_df["control_name_raw"] = mit_df["name"].astype(str)
mit_df = mit_df.loc[mit_df["control_name_raw"].str.strip().ne("")].copy()

# Standardize barrier type
mit_df["control_barrier_type_norm"] = (
    mit_df["barrier_type"]
    .astype(str)
    .str.strip()
    .str.lower()
    .replace({"nan": "unknown", "none": "unknown", "": "unknown"})
)

# -----------------------------
# 2) Create normalized fields
# -----------------------------
mit_df["control_name_norm"] = mit_df["control_name_raw"].map(normalize_control_name)
mit_df["control_tokens"] = mit_df["control_name_norm"].map(tokenize_norm)
mit_df["control_len"] = mit_df["control_tokens"].map(len)

# Drop empty normalized names
mit_df = mit_df.loc[mit_df["control_name_norm"].ne("")].copy()

# -----------------------------
# 3) Normalize barrier_role
# -----------------------------
mit_df["barrier_role_raw"] = mit_df["barrier_role"].astype(str)
mit_df["barrier_role_norm"] = mit_df["barrier_role_raw"].map(normalize_control_name)
mit_df["barrier_role_tokens"] = mit_df["barrier_role_norm"].map(tokenize_norm)
mit_df["barrier_role_len"] = mit_df["barrier_role_tokens"].map(len)

# -----------------------------
# 4) Break out groups by barrier type
# -----------------------------
# A dict of {barrier_type: dataframe_subset}
mit_groups = {
    barrier_type: g.copy()
    for barrier_type, g in mit_df.groupby("control_barrier_type_norm", dropna=False)
}

# Quick sanity prints (optional)
print("Mitigative controls:", len(mit_df))
print("Barrier types:", len(mit_groups))
print("Top barrier types (by count):")
print(mit_df["control_barrier_type_norm"].value_counts().head(10))

"""####Mitigation -Admin Barriers"""

import pandas as pd

mit_admin_dfs = []
for barrier_type in ["administrative", 'ppe']:
    df_subset = mit_groups.get(barrier_type)
    if df_subset is not None:
        mit_admin_dfs.append(df_subset)

if not mit_admin_dfs:
    raise ValueError(
        "No barrier type group found for 'administrative', 'ppe', or 'procedural'. "
        "Check prev_df['control_barrier_type_norm'].unique() for exact values."
    )

mit_admin_df = pd.concat(mit_admin_dfs, ignore_index=True)

print("Administrative prevention rows:", len(admin_df))
print("Unique raw control names:", mit_admin_df["control_name_raw"].nunique())
print("\nTop 25 most frequent raw control names:")
print(mit_admin_df["control_name_raw"].value_counts().head(25))

# Apply family-specific normalization to mit_admin_df
# Explicitly ensure raw name and role columns exist for this specific dataframe
mit_admin_df["control_name_raw"] = mit_admin_df["name"].astype(str)
mit_admin_df["barrier_role_raw"] = mit_admin_df["barrier_role"].astype(str)

mit_admin_df["control_name_family_norm"] = mit_admin_df["control_name_raw"].map(normalize_for_family)
mit_admin_df["barrier_role_family_norm"] = mit_admin_df["barrier_role_raw"].map(normalize_for_family)

# Combine normalized name and role for family matching
mit_admin_df["family_match_text_norm"] = (
    mit_admin_df["control_name_family_norm"].fillna("").astype(str) + " " +
    mit_admin_df["barrier_role_family_norm"].fillna("").astype(str)
).str.strip()
mit_admin_df["family_match_tokens"] = mit_admin_df["family_match_text_norm"].map(to_tokens)

# Now assign match_tokens
mit_admin_df["match_tokens"] = mit_admin_df["family_match_tokens"]

HEAD_NOUNS_MIT_ADMIN = {
    # Incident command / coordination
    "incident", "command", "commander", "ics", "ims", "eoc",
    "coordination", "unified", "liaison", "operations", "planning", "logistics",

    # Emergency response / escalation / abnormal situations
    "emergency", "response", "escalation", "alarm", "alert", "notification", "callout",
    "evacuation", "evacuate", "muster", "accountability", "headcount",
    "exclusion", "zone", "perimeter", "cordon", "access", "control",
    "shelter", "shelterinplace", "sirens", "annunciation",

    # Spill / release / fire / gas response
    "spill", "release", "leak", "gas", "h2s", "fire", "flame", "smoke",
    "containment", "boom", "berm", "bund", "dike", "drain", "sump",
    "cleanup", "recovery", "remediation", "decon", "decontamination",
    "foam", "deluge", "firewater", "hydrant", "extinguisher",

    # Rescue / medical / emergency services interface
    "rescue", "retrieval", "firstaid", "medic", "medical", "triage", "ambulance",
    "firefighter", "hazmat", "mutualaid",

    # Communications during response
    "radio", "comms", "communication", "satphone", "phone", "channel", "handover",

    # Operational response actions (procedural “do now” controls)
    "shutdown", "shut", "shutin", "isolate", "isolation", "depressurize", "depressurization",
    "vent", "venting", "purge", "purging", "inert", "nitrogen",
    "ignition", "source", "grounding", "bonding",

    # Readiness that is tightly tied to response (keep; but narrower than “program/system”)
    "drill", "exercise", "tabletop", "training", "competency",

    # Response-related work control (keep only the response-critical ones)
    "permit", "ptw", "hotwork", "confined", "entry", "atmospheric", "gas", "testing",
    "loto", "lockout", "tagout", "isolationcertificate", "verification",
}


mit_admin_df = mit_admin_df.reset_index(drop=True).copy()
mit_admin_df["block_keys"] = mit_admin_df["match_tokens"].map(lambda toks: block_keys(toks, HEAD_NOUNS_MIT_ADMIN))

block_index = defaultdict(list)
for i, keys in enumerate(mit_admin_df["block_keys"]):
    for k in keys:
        block_index[k].append(i)


LEX_THRESH = 90
PARTIAL_THRESH = 92

pairs = []
seen = set()

names_norm = mit_admin_df["family_match_text_norm"].tolist()
names_raw  = mit_admin_df["control_name_raw"].tolist()

for k, idxs in block_index.items():
    if len(idxs) < 2:
        continue

    idxs_sorted = sorted(idxs)
    for a_pos in range(len(idxs_sorted)):
        i = idxs_sorted[a_pos]
        for b_pos in range(a_pos + 1, len(idxs_sorted)):
            j = idxs_sorted[b_pos]

            key = (i, j)
            if key in seen:
                continue
            seen.add(key)

            s1 = names_norm[i]
            s2 = names_norm[j]

            if len(s1) < 6 or len(s2) < 6:
                continue

            tsr = fuzz.token_set_ratio(s1, s2)
            pr  = fuzz.partial_ratio(s1, s2)

            if tsr >= LEX_THRESH or pr >= PARTIAL_THRESH:
                pairs.append({
                    "i": i,
                    "j": j,
                    "block_key": k,
                    "raw_i": names_raw[i],
                    "raw_j": names_raw[j],
                    "norm_i": s1,
                    "norm_j": s2,
                    "token_set_ratio": tsr,
                    "partial_ratio": pr
                })

pairs_df_admin = pd.DataFrame(pairs).sort_values(
    by=["token_set_ratio", "partial_ratio"],
    ascending=False
)

print("Mitigative admin candidate pairs found:", len(pairs_df_admin))
pairs_df_admin.head(30)

print(mit_admin_df['family_match_text_norm'].tolist())

# ============================================================
# MITIGATIVE-ADMIN FAMILY ASSIGNMENT PIPELINE
# ============================================================

# -----------------------------
# 0) Abbreviation map (admin / procedural / management abbreviations)
# -----------------------------
ABBR_MAP = {
    # Core emergency / incident command
    "erp": "emergency response plan",
    "er": "emergency response",
    "ics": "incident command system",
    "ims": "incident management system",
    "eoc": "emergency operations center",
    "ic": "incident commander",
    "uc": "unified command",

    # Evacuation / personnel accountability
    "evac": "evacuation",
    "muster": "personnel muster",
    "paa": "personnel accountability area",
    "scba": "self contained breathing apparatus",
    "sar": "supplied air respirator",

    # Spill / release / hazmat
    "lopc": "loss of primary containment",
    "h2s": "hydrogen sulfide",
    "hazmat": "hazardous materials response",
    "msds": "material safety data sheet",
    "sds": "safety data sheet",
    "nrc": "national response center",
    "npdes": "national pollutant discharge elimination system",
    "uscg": "united states coast guard",
    "bsee": "bureau of safety and environmental enforcement",

    # Fire / gas / alarms
    "fg": "fire and gas",
    "f&g": "fire and gas",

    # Emergency shutdown / isolation
    "esd": "emergency shutdown",
    "psd": "process shutdown",
    "loto": "lockout tagout",
    "dbb": "double block and bleed",
    "ptw": "permit to work",

    # Response comms
    "sitrep": "situation report",
    "comms": "communications",

    # Rescue / medical
    "cse": "confined space entry",
    "emr": "emergency medical responder",
    "ems": "emergency medical services",
    "cpr": "cardiopulmonary resuscitation",

    # Drills / preparedness
    "tbt": "toolbox talk",
    "dr": "drill",
}

# -----------------------------
# 2) Define MITIGATIVE-ADMIN family taxonomy (single-level) + anchors
# Built from your corpus patterns (subsea interventions, mud/pit monitoring, muster, NRC/BSEE, firewatch,
# shutdown/isolation, medical response, community alerting, investigation/verification).
# -----------------------------
MIT_ADMIN_FAMILIES = {
    # 1) Emergency shutdown / isolation / depressurization (stop/limit escalation)
    "emergency_shutdown_isolation_depressurization": [
        "emergency shutdown", "manual emergency shutdown", "manual facility shutdown",
        "manual platform shut in", "well shut in procedure", "shut in", "fast stop", "blowdown",
        "pipeline pressure bleed down", "bleed down procedure", "depressurization",
        "manual valve closure", "manual closure of overboard discharge line valves",
        "pump shutdown", "stop fluid inflow", "stop discharge", "stop release",
        "all stop procedure", "immediate operations shutdown", "cessation of activities",
        "facility shutdown stop production", "isolation", "isolation and pressure bleed down",
        "manual isolation of power and valves", "dead oil circulation procedure",
        "reduce injection rate", "reduce gas input to column",
    ],

    # 2) Detection / monitoring / surveillance (process + environmental + atmospheric)
    "detection_monitoring_surveillance": [
        "pit level monitoring", "pit volume monitoring", "trip tank monitoring",
        "mud logger monitoring", "mud volume monitoring", "mud pit monitoring",
        "monitoring of fluid return", "monitoring of pit fluid gains",
        "annulus pressure monitoring", "casing pressure monitoring", "packer pressure monitoring",
        "pressure monitoring", "surface pressure identification", "flow checks to detect influx",
        "gas monitoring", "gas level monitoring", "gas leak detection", "visual inspection",
        "sheen detection", "visual sheen detection", "produced water sampling",
        "produced water quality monitoring", "sampling and monitoring",
        "personal gas detection", "personal gas monitors", "hydrogen sulfide monitoring",
        "sensory detection of hydrogen sulfide odor",
        "control room monitoring", "alarm verification", "dispatch of deck operator",
        "unmanned facility operations monitoring",
    ],

    # 3) Active intervention / field actions to stop or limit release (manual/subsea/temporary controls)
    "active_intervention_to_stop_release": [
        "manual intervention", "manual adjustment", "manual level verification and adjustment",
        "manual valve closure intervention", "manual intervention pressurization",
        "lower seal engagement by crew", "subsea intervention vessel dispatch",
        "rov manual valve closure", "rov subsea inspection", "remotely operated vehicle",
        "restore valve to closed position", "stop leak", "containment upon detection",
        "immediate response to leak detection", "investigation and corrective action by derrickman",
    ],

    # 4) Fire / explosion response actions (firewatch + suppression + ignition control used as response control)
    "fire_response_firewatch_ignition_control": [
        "fire team response", "fire suppression equipment", "manual fire suppression",
        "manual fire extinguishing", "fire extinguisher", "correct fire extinguisher selection",
        "fire watch", "firewatch", "fire watch posting", "post suppression monitoring",
        "monitor for secondary incidents", "prevention of re ignition",
        "ignition source control", "ignition source isolation", "hot work procedures",
        "operator response to fire alarm", "turnout gear", "firefighter turnout gear",
        "controlled burn",
    ],

    # 5) Evacuation / muster / shelter / exclusion / access control (merged)
    "evacuation_muster_shelter_exclusion_access_control": [
        "platform evacuation", "evacuation procedures", "emergency evacuation procedures",
        "personnel evacuation", "muster procedures", "personnel muster",
        "personnel accountability", "primary muster alarm", "general alarm and personnel muster",
        "shelter in place", "sheltering in place", "relocation to alternative accommodations",
        "exclusion zone", "evacuation zone establishment", "nonessential personnel exclusion",
        "site security and access control", "access control", "area control",
        "hazardous area assessment", "keep personnel clear", "hose whip zones",
        "public announcement system", "plant evacuation order",
    ],

    # 6) Medical response + evacuation (treated as one operational family for your corpus)
    "medical_response_and_evacuation": [
        "onsite medical response", "rig medic", "medic on site", "field paramedic",
        "first aid", "emergency medical response", "medical treatment",
        "medical evaluation", "telemedicine", "cpr",
        "emergency evacuation and medical response", "medical evacuation",
        "medevac", "medivac", "helicopter", "transport injured personnel",
        "medical clearance",
    ],

    # 7) Environmental response / spill response / cleanup + spill reporting
    "environmental_response_cleanup_reporting": [
        "oil spill response", "spill response", "pollution event", "pollution reporting",
        "sheen detection and reporting", "visual observation of produced water discharge for sheening",
        "platform cleanup procedures", "immediate response and containment upon detection of spill",
        "environmental monitoring", "cleanup", "containment to minimize environmental impact",
        "produced water discharge", "oily water discharge",
        "nrc notification", "national response center", "bsee", "uscg",
        "regulatory reporting", "notify authorities",
    ],

    # 8) Incident command / coordination / communications & escalation (non-regulatory reporting)
    "incident_command_coordination_and_comms": [
        "incident command system", "ics", "incident management team",
        "incident communication and escalation procedures",
        "incident command system and emergency response",
        "coordination with local emergency responders",
        "coordination with fuel pump operations",
        "simultaneous operations", "simops",
        "public announcement system for crew notification",
        "community warning system", "community notification", "external notifications",
        "public health communication", "risk assessment communicate accurate health risk information",
        "situational awareness and risk assessment",
    ],

    # 9) Investigation / corrective actions / post-incident verification & integrity checks
    "investigation_corrective_action_post_incident_verification": [
        "incident investigation", "internal investigation", "root cause analysis",
        "corrective action", "reversion to original formula",
        "damage assessment procedure", "field investigation and inspection",
        "post incident pressure test", "post incident system integrity verification",
        "system integrity verification", "verify pipeline integrity",
        "inspection program implemented post incident",
        "regulatory inspection and oversight",
        "post incident drug testing",
    ],

    # 10) Supervision / staffing / oversight (including contractor oversight & response roles)
    "supervision_staffing_oversight": [
        "operator supervision", "supervision and oversight", "operator staffing",
        "oversight of contractor activities", "ensure contractor compliance",
        "control room monitoring and dispatch", "supervisor", "foreman reporting",
        "landing officer coordination",
    ],

    # 11) Emergency preparedness / response planning / drills / training (readiness bucket)
    "emergency_preparedness_planning_training_drills": [
        "emergency preparedness", "emergency preparedness training and drills",
        "emergency response plan", "emergency action plan", "eap",
        "pre incident planning", "training and drills",
        "hazmat training", "employee training", "driller training and competency assessment",
    ],

    # 12) PPE / respiratory protection / exposure mitigation (kept separate for PPE-heavy controls)
    "ppe_and_respiratory_protection": [
        "personal protective equipment", "ppe",
        "self contained breathing apparatus", "scba", "supplied air respirator", "sar",
        "emergency escape respirator",
        "flame resistant clothing", "fr clothing",
        "safety glasses", "face shield", "gloves",
        "acid resistant clothing", "respiratory protection",
        "personal gas detection equipment",
    ],

    # 13) Permits / controlled work during response (hot work / ignition controls are covered above too)
    "permits_controlled_work_during_response": [
        "permit", "permit to work", "ptw",
        "working at heights permit", "hot work procedures",
        "confined space entry", "cse",
        "ignition source isolation and hot work procedures",
    ],
}

# -----------------------------
# 3) Create multi-anchor embeddings for mitigative-admin families
# -----------------------------
admin_anchor_texts = []
admin_lookup = []
for fam, phrases in MIT_ADMIN_FAMILIES.items():
    for p in phrases:
        admin_anchor_texts.append(p)
        admin_lookup.append(fam)

admin_family_emb = model.encode(admin_anchor_texts, normalize_embeddings=True, show_progress_bar=False)

# -----------------------------
# 4) Conservative rule-first assignment + embedding fallback
# -----------------------------
VALID_ADMIN_FAMILIES = set(MIT_ADMIN_FAMILIES.keys()) | {"other_mit_admin", "unknown"}

def token_contains(tokens: list[str], root: str) -> bool:
    return any(root in t for t in tokens)

def assign_mit_admin_family(text_norm: str, tokens: list[str], min_sim: float = 0.68) -> str:
    if not text_norm or str(text_norm).strip() == "":
        return "unknown"

    # -----------------------------
    # HIGH-PRECISION RULES (ordered intentionally)
    # Designed around *your* observed phrases, with mitigative (response) precedence.
    # -----------------------------

    # 1) Medical response + evacuation
    if (
        token_contains(tokens, "medevac")
        or token_contains(tokens, "medivac")
        or (token_contains(tokens, "medical") and token_contains(tokens, "evac"))
        or token_contains(tokens, "medic")
        or token_contains(tokens, "paramedic")
        or (token_contains(tokens, "first") and token_contains(tokens, "aid"))
        or token_contains(tokens, "cpr")
        or token_contains(tokens, "telemedicine")
        or token_contains(tokens, "triage")
        or token_contains(tokens, "treatment")
        or token_contains(tokens, "stabil")
        or token_contains(tokens, "helicopter")
    ):
        return "medical_response_and_evacuation"

    # 2) Evacuation / muster / shelter / exclusion / access control
    if (
        token_contains(tokens, "evac")
        or token_contains(tokens, "muster")
        or token_contains(tokens, "accountab")
        or token_contains(tokens, "headcount")
        or token_contains(tokens, "shelter")
        or token_contains(tokens, "exclusion")
        or token_contains(tokens, "drop") and token_contains(tokens, "zone")
        or token_contains(tokens, "security")
        or token_contains(tokens, "access") and token_contains(tokens, "control")
        or token_contains(tokens, "hazardous") and token_contains(tokens, "area")
        or token_contains(tokens, "relocat")
        or token_contains(tokens, "nonessential")
        or token_contains(tokens, "public") and token_contains(tokens, "announce")
        or token_contains(tokens, "plant") and token_contains(tokens, "evac")
    ):
        return "evacuation_muster_shelter_exclusion_access_control"

    # 3) Emergency shutdown / isolation / depressurization
    if (
        (token_contains(tokens, "all") and token_contains(tokens, "stop"))
        or token_contains(tokens, "shutdown")
        or (token_contains(tokens, "shut") and token_contains(tokens, "in"))
        or token_contains(tokens, "blowdown")
        or token_contains(tokens, "depressur")
        or (token_contains(tokens, "bleed") and token_contains(tokens, "down"))
        or token_contains(tokens, "isolation")
        or (token_contains(tokens, "valve") and (token_contains(tokens, "close") or token_contains(tokens, "closure")))
        or (token_contains(tokens, "pump") and token_contains(tokens, "shutdown"))
        or (token_contains(tokens, "facility") and token_contains(tokens, "shutdown"))
        or (token_contains(tokens, "platform") and token_contains(tokens, "shut"))
        or (token_contains(tokens, "dead") and token_contains(tokens, "oil") and token_contains(tokens, "circulation"))
        or (token_contains(tokens, "reduce") and (token_contains(tokens, "rate") or token_contains(tokens, "input")))
    ):
        return "emergency_shutdown_isolation_depressurization"

    # 4) Environmental response / spill response / cleanup + spill reporting (NRC/BSEE/USCG)
    if (
        token_contains(tokens, "spill")
        or token_contains(tokens, "pollution")
        or token_contains(tokens, "sheen")
        or token_contains(tokens, "cleanup")
        or (token_contains(tokens, "produced") and token_contains(tokens, "water"))
        or token_contains(tokens, "npdes")
        or token_contains(tokens, "nrc")
        or token_contains(tokens, "bsee")
        or token_contains(tokens, "uscg")
        or (token_contains(tokens, "report") and (token_contains(tokens, "regulator") or token_contains(tokens, "authority") or token_contains(tokens, "notification")))
        or (token_contains(tokens, "notify") and token_contains(tokens, "authorit"))
    ):
        return "environmental_response_cleanup_reporting"

    # 5) Fire response / firewatch / ignition control
    if (
        token_contains(tokens, "firewatch")
        or token_contains(tokens, "extinguish")
        or token_contains(tokens, "extinguisher")
        or (token_contains(tokens, "fire") and token_contains(tokens, "suppression"))
        or (token_contains(tokens, "fire") and token_contains(tokens, "team"))
        or token_contains(tokens, "turnout")
        or token_contains(tokens, "re") and token_contains(tokens, "ignite")
        or (token_contains(tokens, "ignition") and token_contains(tokens, "control"))
        or (token_contains(tokens, "hot") and token_contains(tokens, "work"))
    ):
        return "fire_response_firewatch_ignition_control"

    # 6) Detection / monitoring / surveillance (process + atmospheric + control room)
    if (
        token_contains(tokens, "monitor")
        or token_contains(tokens, "monitoring")
        or token_contains(tokens, "detect")
        or token_contains(tokens, "detection")
        or token_contains(tokens, "observe")
        or token_contains(tokens, "observation")
        or token_contains(tokens, "pit")
        or (token_contains(tokens, "mud") and (token_contains(tokens, "logger") or token_contains(tokens, "volume") or token_contains(tokens, "pit")))
        or (token_contains(tokens, "trip") and token_contains(tokens, "tank"))
        or token_contains(tokens, "annulus")
        or token_contains(tokens, "casing") and token_contains(tokens, "pressure")
        or token_contains(tokens, "packer") and token_contains(tokens, "pressure")
        or token_contains(tokens, "flow") and token_contains(tokens, "check")
        or token_contains(tokens, "return") and token_contains(tokens, "flow")
        or token_contains(tokens, "gas") and (token_contains(tokens, "monitor") or token_contains(tokens, "level") or token_contains(tokens, "leak"))
        or token_contains(tokens, "h2s")
        or (token_contains(tokens, "hydrogen") and token_contains(tokens, "sulfide"))
        or (token_contains(tokens, "control") and token_contains(tokens, "room"))
        or token_contains(tokens, "alarm") and (token_contains(tokens, "verify") or token_contains(tokens, "verification") or token_contains(tokens, "investig"))
        or token_contains(tokens, "unmanned") and token_contains(tokens, "monitor")
    ):
        return "detection_monitoring_surveillance"

    # 7) Active intervention to stop release (manual/subsea/temporary)
    if (
        token_contains(tokens, "intervention")
        or (token_contains(tokens, "manual") and token_contains(tokens, "intervention"))
        or (token_contains(tokens, "manual") and token_contains(tokens, "adjust"))
        or (token_contains(tokens, "manual") and token_contains(tokens, "level"))
        or token_contains(tokens, "rov")
        or token_contains(tokens, "subsea") and (token_contains(tokens, "inspection") or token_contains(tokens, "intervention") or token_contains(tokens, "valve"))
        or token_contains(tokens, "restore") and token_contains(tokens, "closed")
        or token_contains(tokens, "stop") and token_contains(tokens, "leak")
        or token_contains(tokens, "containment") and token_contains(tokens, "upon")
    ):
        return "active_intervention_to_stop_release"

    # 8) Incident command / coordination / comms & escalation
    if (
        token_contains(tokens, "ics")
        or (token_contains(tokens, "incident") and token_contains(tokens, "command"))
        or token_contains(tokens, "coordination")
        or (token_contains(tokens, "incident") and token_contains(tokens, "management") and token_contains(tokens, "team"))
        or token_contains(tokens, "escalat")
        or token_contains(tokens, "communication")
        or token_contains(tokens, "notification") and token_contains(tokens, "external")
        or token_contains(tokens, "community") and (token_contains(tokens, "warning") or token_contains(tokens, "notification") or token_contains(tokens, "alert"))
        or token_contains(tokens, "public") and token_contains(tokens, "health")
    ):
        return "incident_command_coordination_and_comms"

    # 9) Investigation / corrective action / post-incident verification
    if (
        token_contains(tokens, "investig")
        or token_contains(tokens, "root")
        or token_contains(tokens, "correctiv")
        or token_contains(tokens, "cause")
        or (token_contains(tokens, "post") and token_contains(tokens, "incident"))
        or token_contains(tokens, "integrity") and token_contains(tokens, "verif")
        or token_contains(tokens, "pressure") and token_contains(tokens, "test")
        or token_contains(tokens, "damage") and token_contains(tokens, "assessment")
        or token_contains(tokens, "inspection") and token_contains(tokens, "post")
        or token_contains(tokens, "drug") and token_contains(tokens, "test")
    ):
        return "investigation_corrective_action_post_incident_verification"

    # 10) Supervision / staffing / oversight
    if (
        token_contains(tokens, "supervis")
        or token_contains(tokens, "oversight")
        or token_contains(tokens, "staffing")
        or token_contains(tokens, "contractor") and (token_contains(tokens, "oversight") or token_contains(tokens, "supervision"))
        or token_contains(tokens, "foreman")
        or token_contains(tokens, "dispatch")
    ):
        return "supervision_staffing_oversight"

    # 11) Emergency preparedness / planning / training / drills
    if (
        token_contains(tokens, "prepared")
        or token_contains(tokens, "drill")
        or token_contains(tokens, "exercise")
        or token_contains(tokens, "training")
        or token_contains(tokens, "hazmat")
        or token_contains(tokens, "pre") and token_contains(tokens, "incident")
        or token_contains(tokens, "eap")
        or token_contains(tokens, "emergency") and token_contains(tokens, "action") and token_contains(tokens, "plan")
        or token_contains(tokens, "response") and token_contains(tokens, "plan")
    ):
        return "emergency_preparedness_planning_training_drills"

    # 12) PPE / respiratory protection
    if (
        token_contains(tokens, "ppe")
        or token_contains(tokens, "scba")
        or token_contains(tokens, "sar")
        or token_contains(tokens, "respir")
        or token_contains(tokens, "escape") and token_contains(tokens, "respir")
        or token_contains(tokens, "flame") and token_contains(tokens, "resistant")
        or token_contains(tokens, "glasses")
        or token_contains(tokens, "goggl")
        or token_contains(tokens, "face") and token_contains(tokens, "shield")
        or token_contains(tokens, "glove")
    ):
        return "ppe_and_respiratory_protection"

    # 13) Permits / controlled work during response
    if (
        token_contains(tokens, "permit")
        or token_contains(tokens, "ptw")
        or token_contains(tokens, "confined")
        or (token_contains(tokens, "working") and token_contains(tokens, "heights"))
        or (token_contains(tokens, "hot") and token_contains(tokens, "work"))
    ):
        return "permits_controlled_work_during_response"

    # -----------------------------
    # EMBEDDING FALLBACK
    # -----------------------------
    v = model.encode([text_norm], normalize_embeddings=True, show_progress_bar=False)[0]
    sims = admin_family_emb @ v
    best_idx = int(np.argmax(sims))
    best_sim = float(sims[best_idx])

    if best_sim >= min_sim:
        fam = admin_lookup[best_idx]
        return fam if fam in VALID_ADMIN_FAMILIES else "other_mit_admin"

    return "other_mit_admin"


# -----------------------------
# 5) Apply to mit_admin_df (uses combined name+role fields)
# -----------------------------
mit_admin_df["mit_admin_family"] = mit_admin_df.apply(
    lambda r: assign_mit_admin_family(
        text_norm=r.get("family_match_text_norm", ""),
        tokens=r.get("family_match_tokens", []),
        min_sim=0.5
    ),
    axis=1
)

# QA
print(mit_admin_df["mit_admin_family"].value_counts(dropna=False).head(20))
print(mit_admin_df.head())

# --- QA: inspect "other" items and see their nearest anchor ---
other = mit_admin_df[mit_admin_df["mit_admin_family"] == "other_mit_admin"].copy()

if len(other):
    vecs = model.encode(other["family_match_text_norm"].tolist(), normalize_embeddings=True, show_progress_bar=False)
    sims = vecs @ admin_family_emb.T
    best_idx = sims.argmax(axis=1)
    best_sim = sims.max(axis=1)

    other["best_anchor_family"] = [admin_lookup[i] for i in best_idx]
    other["best_anchor_sim"] = best_sim

    display(other[["family_match_text_norm","best_anchor_family","best_anchor_sim"]]
            .sort_values("best_anchor_sim", ascending=False)
            .head(50))

print(mit_admin_df['family_match_text_norm'].tolist())



"""###Mitigations - Engineering Transformation"""

# -----------------------------
# 0) Build mitigative engineering df from grouped barrier types
# -----------------------------
mit_eng_dfs = []
df_subset = mit_groups.get("engineering")
if df_subset is not None:
    mit_eng_dfs.append(df_subset)

if not mit_eng_dfs:
    raise ValueError(
        "No barrier type group found for 'engineering'. "
        "Check prev_df['control_barrier_type_norm'].unique() for exact values."
    )

mit_eng_df = pd.concat(mit_eng_dfs, ignore_index=True)

print("Engineering mitigation rows:", len(mit_eng_df))
print("Unique raw control names:", mit_eng_df["control_name_raw"].nunique() if "control_name_raw" in mit_eng_df else mit_eng_df["name"].astype(str).nunique())
print("\nTop 25 most frequent raw control names:")
print(mit_eng_df["name"].astype(str).value_counts().head(25))

# -----------------------------
# 1) Family-specific normalization for mitigative engineering
# -----------------------------
# Ensure raw columns exist for this dataframe
mit_eng_df["control_name_raw"] = mit_eng_df["name"].astype(str)
mit_eng_df["barrier_role_raw"] = mit_eng_df["barrier_role"].astype(str)

# Use engineering-oriented normalization if available; otherwise fall back to normalize_for_family
try:
    _norm_fn = normalize_for_family_eng  # if you've defined an eng-specific normalizer elsewhere
except NameError:
    _norm_fn = normalize_for_family      # fallback to your existing normalizer

mit_eng_df["control_name_family_norm"] = mit_eng_df["control_name_raw"].map(_norm_fn)
mit_eng_df["barrier_role_family_norm"] = mit_eng_df["barrier_role_raw"].map(_norm_fn)

# Combine normalized name and role for family matching
mit_eng_df["family_match_text_norm"] = (
    mit_eng_df["control_name_family_norm"].fillna("").astype(str) + " " +
    mit_eng_df["barrier_role_family_norm"].fillna("").astype(str)
).str.strip()

mit_eng_df["family_match_tokens"] = mit_eng_df["family_match_text_norm"].map(to_tokens)

# Match tokens used by blocking
mit_eng_df["match_tokens"] = mit_eng_df["family_match_tokens"]

HEAD_NOUNS_ENG = {
    # valves / isolation / relief
    "valve", "psv", "prv", "relief", "isolation", "isolate", "block", "bleed",
    "esd", "shutdown", "sdv", "bop",

    # containment / barriers
    "containment", "secondary", "bund", "berm", "dike", "liner", "drain", "sump",
    "seal", "gasket", "packing", "flange", "coupling", "hose", "fitting",

    # instrumentation / sensing / alarms
    "instrument", "instrumentation", "sensor", "transmitter", "switch", "gauge", "meter",
    "alarm", "annunciator", "trip", "interlock", "logic", "sis", "scada", "plc",

    # detection (gas/fire/etc.)
    "detector", "detection", "gas", "fire", "smoke", "flame", "h2s", "leak",

    # pressure/flow control
    "pressure", "flow", "level", "temperature", "control", "controller", "regulator",

    # mechanical integrity / hardware
    "guard", "shield", "barrier", "enclosure", "housing", "cover", "grating",
    "handrail", "ladder", "platform", "walkway",
    "vent", "venting", "flare", "stack", "rupture", "disk",

    # pumps/compressors/rotating
    "pump", "compressor", "motor", "engine", "turbine", "fan",

    # electrical / grounding
    "grounding", "bonding", "earthing", "breaker", "fuse", "interrupter", "gfci",
    "cable", "wiring", "junction", "panel",

    # structural / piping
    "piping", "pipe", "vessel", "tank", "separator", "header", "manifold",
    "support", "bracket", "clamp",

    # ventilation / HVAC
    "ventilation", "hvac", "exhaust", "intake", "damper",

    # emergency response engineered systems
    "sprinkler", "deluge", "foam", "extinguisher", "suppression", "hydrant",

    # access control / physical (sometimes blended)
    "fence", "gate", "lock", "interlock", "signage",
}

# -----------------------------
# 3) Candidate pair mining via blocking + fuzzy match (same as admin)
# -----------------------------
mit_eng_df = mit_eng_df.reset_index(drop=True).copy()
mit_eng_df["block_keys"] = mit_eng_df["match_tokens"].map(lambda toks: block_keys(toks, HEAD_NOUNS_ENG))

block_index = defaultdict(list)
for i, keys in enumerate(mit_eng_df["block_keys"]):
    for k in keys:
        block_index[k].append(i)

LEX_THRESH = 90
PARTIAL_THRESH = 92

pairs = []
seen = set()

names_norm = mit_eng_df["family_match_text_norm"].tolist()
names_raw  = mit_eng_df["control_name_raw"].tolist()

for k, idxs in block_index.items():
    if len(idxs) < 2:
        continue

    idxs_sorted = sorted(idxs)
    for a_pos in range(len(idxs_sorted)):
        i = idxs_sorted[a_pos]
        for b_pos in range(a_pos + 1, len(idxs_sorted)):
            j = idxs_sorted[b_pos]

            key = (i, j)
            if key in seen:
                continue
            seen.add(key)

            s1 = names_norm[i]
            s2 = names_norm[j]

            if len(s1) < 6 or len(s2) < 6:
                continue

            tsr = fuzz.token_set_ratio(s1, s2)
            pr  = fuzz.partial_ratio(s1, s2)

            if tsr >= LEX_THRESH or pr >= PARTIAL_THRESH:
                pairs.append({
                    "i": i,
                    "j": j,
                    "block_key": k,
                    "raw_i": names_raw[i],
                    "raw_j": names_raw[j],
                    "norm_i": s1,
                    "norm_j": s2,
                    "token_set_ratio": tsr,
                    "partial_ratio": pr
                })

pairs_df_eng = pd.DataFrame(pairs).sort_values(
    by=["token_set_ratio", "partial_ratio"],
    ascending=False
)

print("Mitigative engineering candidate pairs found:", len(pairs_df_eng))
pairs_df_eng.head(30)

# Abbreviation map (engineering/instrument abbreviations)
ABBR_MAP_ENG = {
    "psv": "pressure safety valve",
    "prv": "pressure relief valve",
    "esd": "emergency shutdown",
    "sdv": "shutdown valve",
    "bop": "blowout preventer",
    "scada": "supervisory control and data acquisition",
    "plc": "programmable logic controller",
    "sis": "safety instrumented system",
    "psd": "process shutdown",
    "cctv": "closed circuit television",
    "ppe": "personal protective equipment",
    "lopc": "loss of primary containment",
    "h2s": "hydrogen sulfide",
    "hse": "health safety environment",
    "pfd": "process flow diagram",
    "pid": "piping and instrumentation diagram",
    "hvac": "heating ventilation and air conditioning",
    "f&amp;g": "fire and gas",
    "fg": "fire gas",
}

MIT_ENG_FAMILIES = {
    "gas_detection_atmospheric_monitoring": [
        "gas detection system", "platform gas detection system", "fire and gas detection",
        "hydrocarbon gas detection", "gas detector", "portable gas detector",
        "portable gas detection monitoring", "atmospheric monitoring devices",
        "oxygen deficient atmosphere monitoring", "rov investigation and leak detection",
        "rov monitoring and intervention", "detect leak", "leak detection"
    ],

    "alarms_general_alarm_pa": [
        "platform alarm system", "general alarm system", "visible and audible alarm system",
        "alarm system alert", "initiate alarms", "muster alarm", "alert personnel to emergency"
    ],

    "emergency_shutdown_isolation": [
        "emergency shutdown system", "emergency shut down", "emergency shutdown systems",
        "manual emergency shutdown", "automatic isolation and shutdown",
        "shutdown of mud pumps", "mud pumps shutdown", "pipeline isolation system",
        "remote isolation devices", "remotely operated emergency isolation valves",
        "emergency isolation valves", "isolate gas source", "stop flow"
    ],

    "emergency_disconnect_eds": [
        "emergency disconnect system", "eds", "emergency disconnect",
        "disconnect riser", "emergency disconnect to prevent damage"
    ],

    "well_control_barriers_kill": [
        "blowout preventer", "bop", "blind shear rams", "shear rams",
        "seal wellbore", "prevent loss of well control",
        "cement isolation barrier", "cement plug", "bridge plug installation",
        "crown valve closure", "lower master valve closure",
        "well kill operations", "regain well control", "mechanical isolation of pressure source"
    ],

    "pressure_relief_blowdown_flare_disposal": [
        "blowdown drum", "blowdown drum and stack", "stack disposal system",
        "flare system", "safely combust", "atmospheric knockout drums",
        "knockout drum", "relief discharge", "pressure bleed off", "choke manifold pressure bleed off",
        "emergency pressure control systems", "two phase discharge"
    ],

    "ignition_source_control": [
        "ignition source control", "prevent ignition of vapor cloud",
        "prevent ignition of released hydrocarbons", "electrical isolation"
    ],

    "active_fire_protection_firefighting": [
        "fire suppression equipment", "fire hose", "extinguishers",
        "fire protection systems", "fire containment and suppression",
        "fire fighting equipment", "fire water system", "deluge", "suppression"
    ],

    "passive_fire_blast_protection": [
        "firewall separation", "blast wall", "physical barriers shield",
        "structural protection", "building structural integrity to withstand explosion",
        "control room siting and structural protection", "storage magazine containment",
        "explosion safeguards", "explosion protection system", "esp explosion safeguards"
    ],

    "control_room_habitability_hvac_pressurization": [
        "hvac system for control room", "positive pressure control room design",
        "contaminant free air inlet", "exhaust vents", "prevent toxic gas ingress",
        "gas presence in control room area", "emergency shutdown and ventilation system",
        "room and emergency ventilation systems", "isolate and ventilate control room"
    ],

    "emergency_power_backup_utilities": [
        "emergency generator auto start", "backup power during emergency shutdown",
        "emergency backup power", "emergency backup water supply system",
        "alternate water source", "water supply for hf mitigation"
    ],

    "spill_containment_environmental_mitigation": [
        "spill containment system", "secondary containment", "berm",
        "absorbent booms", "contain and limit spread of oil spill",
        "oil spill response resources", "skimming vessel", "environmental protection"
    ],

    "chemical_release_scrubbing_neutralization": [
        "emergency scrubbing units", "backup scrubbing capacity",
        "hf water mitigation system", "capture and neutralize released hf",
        "caustic addition to increase ph", "stop bleach decomposition reaction"
    ],

    "physical_protection_retention_restraints": [
        "safety restraints", "whip checks", "high pressure hoses",
        "bottle rack with securing mechanisms", "aluminum bottle rack containment",
        "stanchions bolted", "secondary retention system", "piping support clamps",
        "shield personnel and equipment from flying debris", "barricades around nitrogen filling area"
    ],

    "emergency_escape_access_rescue_decon": [
        "emergency evacuation system", "platform access ladder",
        "personnel basket for safe transfer of injured worker",
        "rescue equipment setup at confined space location",
        "eyewash shower station", "decontaminate personnel"
    ],

    "structural_mechanical_integrity_escalation_prevention": [
        "piping integrity prevent secondary release", "ammonia refrigeration system piping integrity",
        "equipment capacity and load rating compliance", "design to withstand expected loads",
        "immediate area securing and hazard mitigation prevent secondary incidents"
    ],

    "remote_monitoring_intervention_subsea": [
        "rov inspection of subsea equipment", "rov investigation", "subsea equipment assess integrity",
        "remote operated vehicle monitoring and intervention", "confirm and locate gas release"
    ],

    "marine_collision_avoidance": [
        "navigation aid lights prevent vessel collision", "prevent vessel collision with platform"
    ],
}


model = SentenceTransformer("all-MiniLM-L6-v2")

eng_anchor_texts = []
eng_lookup = []
for fam, phrases in MIT_ENG_FAMILIES.items():
    for p in phrases:
        eng_anchor_texts.append(p)
        eng_lookup.append(fam)

eng_family_emb = model.encode(eng_anchor_texts, normalize_embeddings=True, show_progress_bar=False)

VALID_ENG_FAMILIES = set(MIT_ENG_FAMILIES.keys()) | {"unknown"}

def token_contains(tokens: list[str], root: str) -> bool:
    return any(root in t for t in tokens)

def assign_mit_eng_family(text_norm: str, tokens: list[str], min_sim: float = 0.68) -> str:
    if not text_norm or str(text_norm).strip() == "":
        return "unknown"

    # -----------------------------
    # HIGH-PRECISION RULES (ordered intentionally)
    # Map directly to MIT_ENG_FAMILIES keys
    # -----------------------------

    # 1) Emergency disconnect / EDS
    if (
        token_contains(tokens, "eds")
        or (token_contains(tokens, "emergency") and token_contains(tokens, "disconnect"))
        or (token_contains(tokens, "disconnect") and token_contains(tokens, "riser"))
    ):
        return "emergency_disconnect_eds"

    # 2) Well control barriers / kill (BOP, shear rams, plugs, kill ops)
    if (
        token_contains(tokens, "bop")
        or (token_contains(tokens, "blowout") and token_contains(tokens, "prevent"))
        or (token_contains(tokens, "shear") and token_contains(tokens, "ram"))
        or (token_contains(tokens, "blind") and token_contains(tokens, "ram"))
        or (token_contains(tokens, "cement") and token_contains(tokens, "plug"))
        or token_contains(tokens, "bridge") and token_contains(tokens, "plug")
        or (token_contains(tokens, "crown") and token_contains(tokens, "valve"))
        or (token_contains(tokens, "master") and token_contains(tokens, "valve"))
        or token_contains(tokens, "well") and token_contains(tokens, "kill")
    ):
        return "well_control_barriers_kill"

    # 3) Emergency shutdown / isolation (ESD, remote isolation valves, pipeline isolation, pump shutdown)
    if (
        token_contains(tokens, "esd")
        or (token_contains(tokens, "emergency") and token_contains(tokens, "shutdown"))
        or (token_contains(tokens, "manual") and token_contains(tokens, "shutdown"))
        or token_contains(tokens, "shutdown")
        or (token_contains(tokens, "pipeline") and token_contains(tokens, "isolation"))
        or (token_contains(tokens, "remote") and token_contains(tokens, "isolation"))
        or (token_contains(tokens, "isolation") and token_contains(tokens, "valve"))
        or (token_contains(tokens, "mud") and token_contains(tokens, "pump") and token_contains(tokens, "shutdown"))
        or (token_contains(tokens, "choke") and token_contains(tokens, "closure"))
        or (token_contains(tokens, "isolate") and (token_contains(tokens, "source") or token_contains(tokens, "inventory")))
    ):
        return "emergency_shutdown_isolation"

    # 4) Pressure relief / blowdown / flare / disposal (KO drum, blowdown stack, choke bleed-off)
    if (
        token_contains(tokens, "blowdown")
        or token_contains(tokens, "flare")
        or (token_contains(tokens, "knockout") and token_contains(tokens, "drum"))
        or (token_contains(tokens, "disposal") and (token_contains(tokens, "stack") or token_contains(tokens, "system")))
        or (token_contains(tokens, "relief") and token_contains(tokens, "discharge"))
        or (token_contains(tokens, "pressure") and token_contains(tokens, "bleed"))
        or (token_contains(tokens, "choke") and token_contains(tokens, "bleed"))
        or (token_contains(tokens, "two") and token_contains(tokens, "phase") and token_contains(tokens, "discharge"))
    ):
        return "pressure_relief_blowdown_flare_disposal"

    # 5) Gas detection / atmospheric monitoring (incl. O2 deficiency, fixed+portable)
    if (
        token_contains(tokens, "detector")
        or token_contains(tokens, "detect")
        or token_contains(tokens, "gas") and token_contains(tokens, "detection")
        or token_contains(tokens, "fire") and token_contains(tokens, "gas") and token_contains(tokens, "detection")
        or token_contains(tokens, "oxygen") and token_contains(tokens, "deficien")
        or token_contains(tokens, "atmospheric") and token_contains(tokens, "monitor")
        or token_contains(tokens, "portable") and token_contains(tokens, "gas")
    ):
        return "gas_detection_atmospheric_monitoring"

    # 6) Alarms / general alarm / audible-visible notification
    if (
        token_contains(tokens, "alarm")
        or (token_contains(tokens, "audible") and token_contains(tokens, "visible"))
        or (token_contains(tokens, "general") and token_contains(tokens, "alarm"))
        or (token_contains(tokens, "muster") and token_contains(tokens, "alarm"))
        or (token_contains(tokens, "alert") and token_contains(tokens, "personnel"))
    ):
        return "alarms_general_alarm_pa"

    # 7) Ignition source control
    if (
        token_contains(tokens, "ignition")
        or (token_contains(tokens, "source") and token_contains(tokens, "control"))
        or (token_contains(tokens, "prevent") and token_contains(tokens, "ignition"))
        or (token_contains(tokens, "vapor") and token_contains(tokens, "cloud"))
    ):
        return "ignition_source_control"

    # 8) Active fire protection / firefighting systems (firewater, deluge, extinguishers)
    if (
        token_contains(tokens, "firewater")
        or (token_contains(tokens, "fire") and token_contains(tokens, "water"))
        or token_contains(tokens, "deluge")
        or token_contains(tokens, "sprink")
        or token_contains(tokens, "foam")
        or token_contains(tokens, "hydrant")
        or token_contains(tokens, "extinguish")
        or token_contains(tokens, "fire") and token_contains(tokens, "hose")
        or token_contains(tokens, "fire") and token_contains(tokens, "suppression")
        or token_contains(tokens, "fire") and token_contains(tokens, "fighting")
    ):
        return "active_fire_protection_firefighting"

    # 9) Passive fire/blast protection (firewall, blast/explosion structural protection)
    if (
        token_contains(tokens, "firewall")
        or token_contains(tokens, "blast")
        or token_contains(tokens, "explosion") and (token_contains(tokens, "protection") or token_contains(tokens, "safeguard"))
        or token_contains(tokens, "structural") and token_contains(tokens, "integrity")
        or token_contains(tokens, "storage") and token_contains(tokens, "magazine")
        or token_contains(tokens, "containment") and token_contains(tokens, "explosion")
        or token_contains(tokens, "debris") and token_contains(tokens, "shield")
    ):
        return "passive_fire_blast_protection"

    # 10) Control room habitability / HVAC / pressurization / ventilation
    if (
        token_contains(tokens, "hvac")
        or (token_contains(tokens, "control") and token_contains(tokens, "room") and (token_contains(tokens, "pressur") or token_contains(tokens, "habit") or token_contains(tokens, "siting")))
        or (token_contains(tokens, "positive") and token_contains(tokens, "pressure"))
        or (token_contains(tokens, "air") and token_contains(tokens, "inlet"))
        or (token_contains(tokens, "contaminant") and token_contains(tokens, "free"))
        or (token_contains(tokens, "ventilation") and (token_contains(tokens, "emergency") or token_contains(tokens, "room")))
        or (token_contains(tokens, "toxic") and token_contains(tokens, "ingress"))
    ):
        return "control_room_habitability_hvac_pressurization"

    # 11) Emergency power / backup utilities (generators, backup water)
    if (
        token_contains(tokens, "generator")
        or (token_contains(tokens, "backup") and token_contains(tokens, "power"))
        or (token_contains(tokens, "emergency") and token_contains(tokens, "power"))
        or (token_contains(tokens, "backup") and token_contains(tokens, "water"))
        or (token_contains(tokens, "water") and token_contains(tokens, "supply") and token_contains(tokens, "backup"))
        or (token_contains(tokens, "alternate") and token_contains(tokens, "water"))
    ):
        return "emergency_power_backup_utilities"

    # 12) Spill containment / environmental mitigation hardware (berms, booms, skimming)
    if (
        token_contains(tokens, "spill") and token_contains(tokens, "contain")
        or token_contains(tokens, "secondary") and token_contains(tokens, "contain")
        or token_contains(tokens, "berm")
        or token_contains(tokens, "bund")
        or token_contains(tokens, "dike")
        or token_contains(tokens, "boom")
        or token_contains(tokens, "absorbent")
        or token_contains(tokens, "skimming")
        or token_contains(tokens, "skimmer")
        or token_contains(tokens, "environment")
    ):
        return "spill_containment_environmental_mitigation"

    # 13) Chemical scrubbing / neutralization / HF mitigation
    if (
        token_contains(tokens, "scrubb")
        or token_contains(tokens, "hf")
        or (token_contains(tokens, "water") and token_contains(tokens, "mitigation"))
        or (token_contains(tokens, "neutral") and token_contains(tokens, "release"))
        or (token_contains(tokens, "caustic") and token_contains(tokens, "ph"))
        or (token_contains(tokens, "bleach") and token_contains(tokens, "decomposition"))
    ):
        return "chemical_release_scrubbing_neutralization"

    # 14) Physical protection / retention / restraints (whip checks, bottle racks, retention systems, barricades)
    if (
        token_contains(tokens, "whip") and token_contains(tokens, "check")
        or token_contains(tokens, "restraint")
        or token_contains(tokens, "hose") and token_contains(tokens, "restraint")
        or token_contains(tokens, "bottle") and token_contains(tokens, "rack")
        or token_contains(tokens, "secondary") and token_contains(tokens, "retention")
        or token_contains(tokens, "stanchion")
        or token_contains(tokens, "connex")
        or token_contains(tokens, "container") and token_contains(tokens, "shield")
        or token_contains(tokens, "flying") and token_contains(tokens, "debris")
        or token_contains(tokens, "barricade")
    ):
        return "physical_protection_retention_restraints"

    # 15) Emergency escape/access/rescue/decon hardware (eyewash, ladders, rescue gear, personnel basket)
    if (
        token_contains(tokens, "eyewash")
        or token_contains(tokens, "shower")
        or token_contains(tokens, "decontam")
        or token_contains(tokens, "ladder")
        or token_contains(tokens, "access") and token_contains(tokens, "ladder")
        or token_contains(tokens, "rescue") and token_contains(tokens, "equipment")
        or token_contains(tokens, "confined") and token_contains(tokens, "rescue")
        or token_contains(tokens, "personnel") and token_contains(tokens, "basket")
        or token_contains(tokens, "emergency") and token_contains(tokens, "evacuation") and token_contains(tokens, "system")
    ):
        return "emergency_escape_access_rescue_decon"

    # 16) Structural/mechanical integrity for escalation prevention (post-incident secondary failure prevention)
    if (
        token_contains(tokens, "piping") and token_contains(tokens, "integrity")
        or token_contains(tokens, "support") and token_contains(tokens, "clamp")
        or token_contains(tokens, "piping") and token_contains(tokens, "support")
        or token_contains(tokens, "load") and token_contains(tokens, "rating")
        or token_contains(tokens, "capacity") and token_contains(tokens, "rating")
        or token_contains(tokens, "ammonia") and token_contains(tokens, "piping")
    ):
        return "structural_mechanical_integrity_escalation_prevention"

    # 17) Remote monitoring/intervention (ROV/subsea)
    if (
        token_contains(tokens, "rov")
        or token_contains(tokens, "subsea")
        or (token_contains(tokens, "remote") and token_contains(tokens, "operated") and token_contains(tokens, "vehicle"))
        or (token_contains(tokens, "remote") and token_contains(tokens, "monitor"))
        or (token_contains(tokens, "locate") and token_contains(tokens, "release"))
    ):
        return "remote_monitoring_intervention_subsea"

    # 18) Marine collision avoidance aids
    if (
        token_contains(tokens, "navigation") and token_contains(tokens, "light")
        or token_contains(tokens, "collision")
        or token_contains(tokens, "vessel") and token_contains(tokens, "collision")
    ):
        return "marine_collision_avoidance"

    # -----------------------------
    # EMBEDDING FALLBACK
    # -----------------------------
    v = model.encode([text_norm], normalize_embeddings=True, show_progress_bar=False)[0]
    sims = eng_family_emb @ v
    best_idx = int(np.argmax(sims))
    best_sim = float(sims[best_idx])

    if best_sim >= min_sim:
        fam = eng_lookup[best_idx]
        return fam if fam in VALID_ENG_FAMILIES else "other_mit_engineering"

    return "other_mit_engineering"

# Apply to mit_eng_df
mit_eng_df["mit_eng_family"] = mit_eng_df.apply(
    lambda r: assign_mit_eng_family(
        text_norm=r.get("family_match_text_norm", ""),
        tokens=r.get("family_match_tokens", []),
        min_sim=0.4
    ),
    axis=1
)

# QA
print(mit_eng_df["mit_eng_family"].value_counts(dropna=False).head(20))
print(mit_eng_df.head())

# --- QA: inspect "other" items and see their nearest anchor ---
other = mit_eng_df[mit_eng_df["mit_eng_family"] == "other_mit_engineering"].copy()

if len(other):
    vecs = model.encode(other["family_match_text_norm"].tolist(), normalize_embeddings=True, show_progress_bar=False)
    sims = vecs @ eng_family_emb.T
    best_idx = sims.argmax(axis=1)
    best_sim = sims.max(axis=1)

    other["best_anchor_family"] = [admin_lookup[i] for i in best_idx]
    other["best_anchor_sim"] = best_sim

    display(other[["family_match_text_norm","best_anchor_family","best_anchor_sim"]]
            .sort_values("best_anchor_sim", ascending=False)
            .head(50))

print(mit_eng_df['family_match_text_norm'].tolist())

admin_df.info()

engineering_df.info()

print(mit_admin_df.info())

print(mit_eng_df.info())

"""##Combining DFs"""

# --- MITIGATION ENGINEERING ---
mit_eng_df = mit_eng_df.copy()
mit_eng_df["barrier_level"] = "mitigation"
mit_eng_df["barrier_type"] = "engineering"
mit_eng_df["barrier_family"] = mit_eng_df["mit_eng_family"]


# --- MITIGATION ADMIN ---
mit_admin_df = mit_admin_df.copy()
mit_admin_df["barrier_level"] = "mitigation"
mit_admin_df["barrier_type"] = "administrative"
mit_admin_df["barrier_family"] = mit_admin_df["mit_admin_family"]


# --- PREVENTION ENGINEERING ---
engineering_df = engineering_df.copy()
engineering_df["barrier_level"] = "prevention"
engineering_df["barrier_type"] = "engineering"
engineering_df["barrier_family"] = engineering_df["eng_family"]


# --- PREVENTION ADMIN ---
admin_df = admin_df.copy()
admin_df["barrier_level"] = "prevention"
admin_df["barrier_type"] = "administrative"
admin_df["barrier_family"] = admin_df["admin_family"]

COMMON_COLS = [
    "incident_id",
    'control_id',
    'control_name_raw',
    "control_name_norm",
    "barrier_role_raw",
    "barrier_role_norm",
    "family_match_text_norm",
    "barrier_level",
    "barrier_type",
    "barrier_family",
    'line_of_defense',
    'lod_basis',
    'barrier_status',
    'barrier_failed',
    'human_contribution_value',
    'barrier_failed_human',
    'confidence',
    'supporting_text_count',
    'source_agency',
    'provider_bucket',
    'json_path',
]

mit_eng_df      = mit_eng_df[COMMON_COLS]
mit_admin_df    = mit_admin_df[COMMON_COLS]
engineering_df  = engineering_df[COMMON_COLS]
admin_df        = admin_df[COMMON_COLS]

all_barriers_df = pd.concat(
    [mit_eng_df, mit_admin_df, engineering_df, admin_df],
    ignore_index=True
)

print(all_barriers_df.shape)
print(all_barriers_df["barrier_level"].value_counts())
print(all_barriers_df["barrier_type"].value_counts())
print(all_barriers_df["barrier_family"].nunique())

all_barriers_df.to_csv('normalized_dfV1.csv')